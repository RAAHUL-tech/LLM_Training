{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Create Folders","metadata":{}},{"cell_type":"code","source":"import os\nos.makedirs('configs', exist_ok=True)\nos.makedirs('train', exist_ok=True)\nos.makedirs('inference', exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T23:40:13.739635Z","iopub.execute_input":"2025-12-21T23:40:13.739917Z","iopub.status.idle":"2025-12-21T23:40:13.744373Z","shell.execute_reply.started":"2025-12-21T23:40:13.739893Z","shell.execute_reply":"2025-12-21T23:40:13.743532Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"!pip install wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T23:11:18.146674Z","iopub.execute_input":"2025-12-21T23:11:18.147460Z","iopub.status.idle":"2025-12-21T23:11:21.214885Z","shell.execute_reply.started":"2025-12-21T23:11:18.147421Z","shell.execute_reply":"2025-12-21T23:11:21.214153Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.22.2)\nRequirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.1)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.1)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.12.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.5)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.42.1)\nRequirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.11.12)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\n\n# Load W&B key from Kaggle Secrets\nos.environ[\"WANDB_API_KEY\"] = UserSecretsClient().get_secret(\"WANDB_API_KEY\")\nos.environ[\"WANDB_PROJECT\"] = \"llm-from-scratch\"\nos.environ[\"WANDB_ENTITY\"] = \"rahulkrish28-california-state-university-fullerton\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T23:11:22.608933Z","iopub.execute_input":"2025-12-21T23:11:22.609720Z","iopub.status.idle":"2025-12-21T23:11:22.690805Z","shell.execute_reply.started":"2025-12-21T23:11:22.609685Z","shell.execute_reply":"2025-12-21T23:11:22.690077Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"%%writefile configs/train_base.yaml\n\nmodel_name: gpt2\ndataset_name: wikitext\ndataset_config: wikitext-2-raw-v1\n\nmax_length: 512\nbatch_size: 2\nnum_epochs: 1\n\nlearning_rate: 5e-5\nweight_decay: 0.01\nwarmup_ratio: 0.1\nmax_grad_norm: 1.0\n\noutput_dir: outputs/train_lm\nseed: 42","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T23:11:25.359823Z","iopub.execute_input":"2025-12-21T23:11:25.360203Z","iopub.status.idle":"2025-12-21T23:11:25.365290Z","shell.execute_reply.started":"2025-12-21T23:11:25.360177Z","shell.execute_reply":"2025-12-21T23:11:25.364642Z"}},"outputs":[{"name":"stdout","text":"Writing configs/train_base.yaml\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"%%writefile train/train_lm.py\n\nimport math\nimport yaml\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    DataCollatorForLanguageModeling,\n    get_scheduler\n)\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n# -------------------------\n# Load config\n# -------------------------\nwith open(\"configs/train_base.yaml\") as f:\n    cfg = yaml.safe_load(f)\n\ntorch.manual_seed(cfg[\"seed\"])\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# -------------------------\n# Tokenizer & Model\n# -------------------------\ntokenizer = AutoTokenizer.from_pretrained(cfg[\"model_name\"])\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(cfg[\"model_name\"])\nmodel.to(device)\n\n# -------------------------\n# Dataset\n# -------------------------\ndataset = load_dataset(\n    cfg[\"dataset_name\"],\n    cfg[\"dataset_config\"]\n)\n\ndef tokenize_fn(examples):\n    tokens = tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        max_length=cfg[\"max_length\"],\n        padding=False,\n    )\n\n    # FILTER EMPTY SEQUENCES\n    input_ids = []\n    for ids in tokens[\"input_ids\"]:\n        if len(ids) > 0:\n            input_ids.append(ids)\n\n    return {\"input_ids\": input_ids}\n\ntokenized = dataset.map(\n    tokenize_fn,\n    batched=True,\n    remove_columns=[\"text\"]\n)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\ntrain_loader = DataLoader(\n    tokenized[\"train\"],\n    batch_size=cfg[\"batch_size\"],\n    shuffle=True,\n    collate_fn=data_collator\n)\n\n# -------------------------\n# Optimizer\n# -------------------------\n# ---- Defensive casting ----\ncfg[\"learning_rate\"] = float(cfg[\"learning_rate\"])\ncfg[\"weight_decay\"] = float(cfg[\"weight_decay\"])\n\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=cfg[\"learning_rate\"],\n    weight_decay=cfg[\"weight_decay\"]\n)\n\nnum_training_steps = cfg[\"num_epochs\"] * len(train_loader)\nlr_scheduler = get_scheduler(\n    name=\"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps\n)\n\ndef sanity_check(batch):\n    assert batch[\"input_ids\"].dim() == 2\n    assert batch[\"input_ids\"].size(1) > 0\n    \n# -------------------------\n# Training Loop\n# -------------------------\nmodel.train()\nprogress = tqdm(range(num_training_steps))\n\nfor epoch in range(cfg[\"num_epochs\"]):\n    for batch in train_loader:\n        sanity_check(batch)\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        outputs = model(**batch)\n        loss = outputs.loss\n\n        loss.backward()\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\n        progress.update(1)\n        progress.set_postfix(loss=loss.item())\n\n# -------------------------\n# Perplexity\n# -------------------------\nppl = math.exp(loss.item())\nprint(f\"Final Perplexity: {ppl:.2f}\")\n\nmodel.save_pretrained(cfg[\"output_dir\"])\ntokenizer.save_pretrained(cfg[\"output_dir\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T23:11:28.605352Z","iopub.execute_input":"2025-12-21T23:11:28.605901Z","iopub.status.idle":"2025-12-21T23:11:28.611238Z","shell.execute_reply.started":"2025-12-21T23:11:28.605874Z","shell.execute_reply":"2025-12-21T23:11:28.610577Z"}},"outputs":[{"name":"stdout","text":"Writing train/train_lm.py\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!python train/train_lm.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T23:11:32.674217Z","iopub.execute_input":"2025-12-21T23:11:32.674896Z","iopub.status.idle":"2025-12-21T23:39:39.812817Z","shell.execute_reply.started":"2025-12-21T23:11:32.674870Z","shell.execute_reply":"2025-12-21T23:39:39.811953Z"}},"outputs":[{"name":"stdout","text":"2025-12-21 23:11:45.242640: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766358705.429914     113 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766358705.482356     113 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766358705.928892     113 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766358705.928939     113 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766358705.928943     113 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766358705.928947     113 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\ntokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26.0/26.0 [00:00<00:00, 125kB/s]\nconfig.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 665/665 [00:00<00:00, 5.22MB/s]\nvocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.04M/1.04M [00:00<00:00, 12.9MB/s]\nmerges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00<00:00, 9.25MB/s]\ntokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.36M/1.36M [00:00<00:00, 44.1MB/s]\nmodel.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 548M/548M [00:02<00:00, 273MB/s]\ngeneration_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 124/124 [00:00<00:00, 953kB/s]\nREADME.md: 10.5kB [00:00, 42.0MB/s]\nwikitext-2-raw-v1/test-00000-of-00001.pa(â€¦): 100%|â–ˆ| 733k/733k [00:00<00:00, 867\nwikitext-2-raw-v1/train-00000-of-00001.p(â€¦): 100%|â–ˆ| 6.36M/6.36M [00:00<00:00, 8\nwikitext-2-raw-v1/validation-00000-of-00(â€¦): 100%|â–ˆ| 657k/657k [00:00<00:00, 1.7\nGenerating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4358/4358 [00:00<00:00, 97214.61 examples/s]\nGenerating train split: 100%|â–ˆâ–ˆ| 36718/36718 [00:00<00:00, 730511.59 examples/s]\nGenerating validation split: 100%|â–ˆ| 3760/3760 [00:00<00:00, 623072.30 examples/\nMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4358/4358 [00:00<00:00, 11133.00 examples/s]\nMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36718/36718 [00:03<00:00, 9759.03 examples/s]\nMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3760/3760 [00:00<00:00, 11489.25 examples/s]\n  0%|                                                 | 0/11884 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11884/11884 [27:23<00:00,  9.48it/s, loss=2.66]Final Perplexity: 14.24\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11884/11884 [27:24<00:00,  7.22it/s, loss=2.66]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"%%writefile configs/inference.yaml\n\n# =========================\n# Inference Configuration\n# =========================\n\n# -------------------------\n# Model\n# -------------------------\nmodel_dir: \"outputs/train_lm\"   # same as cfg[\"output_dir\"] during training\ndevice: \"auto\"                  # auto | cuda | cpu\n\n# -------------------------\n# Tokenization\n# -------------------------\nmax_input_length: 512           # truncate prompt if longer than this\n\n# -------------------------\n# Generation Parameters\n# -------------------------\nmax_new_tokens: 150             # number of tokens to generate\ndo_sample: true                 # sampling vs greedy decoding\n\ntemperature: 0.8                # randomness (1.0 = neutral)\ntop_p: 0.9                      # nucleus sampling\ntop_k: 50                       # optional, can be null\nrepetition_penalty: 1.0         # >1.0 reduces repetition\n\n# -------------------------\n# Special Tokens\n# -------------------------\npad_token: \"eos\"                # use EOS as PAD\nskip_special_tokens: true\n\n# -------------------------\n# Runtime\n# -------------------------\nuse_fp16: false                 # set true if GPU supports fp16\nbatch_size: 1                   # for batch inference","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T23:39:45.578393Z","iopub.execute_input":"2025-12-21T23:39:45.578688Z","iopub.status.idle":"2025-12-21T23:39:45.584148Z","shell.execute_reply.started":"2025-12-21T23:39:45.578656Z","shell.execute_reply":"2025-12-21T23:39:45.583457Z"}},"outputs":[{"name":"stdout","text":"Writing configs/inference.yaml\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"%%writefile inference/inference.py\n\nimport yaml\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# -------------------------\n# Load Inference Config\n# -------------------------\nwith open(\"configs/inference.yaml\") as f:\n    cfg = yaml.safe_load(f)\n\n# -------------------------\n# Device\n# -------------------------\nif cfg[\"device\"] == \"auto\":\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nelse:\n    device = cfg[\"device\"]\n\n# -------------------------\n# Tokenizer & Model\n# -------------------------\ntokenizer = AutoTokenizer.from_pretrained(cfg[\"model_dir\"])\n\n# Pad token handling\nif cfg[\"pad_token\"] == \"eos\":\n    tokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(cfg[\"model_dir\"])\n\nif cfg.get(\"use_fp16\", False) and device == \"cuda\":\n    model = model.half()\n\nmodel.to(device)\nmodel.eval()\n\n# -------------------------\n# Generation Function\n# -------------------------\n@torch.no_grad()\ndef generate(prompt: str):\n    inputs = tokenizer(\n        prompt,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=cfg[\"max_input_length\"],\n    ).to(device)\n\n    generation_kwargs = {\n        \"max_new_tokens\": cfg[\"max_new_tokens\"],\n        \"do_sample\": cfg[\"do_sample\"],\n        \"temperature\": cfg[\"temperature\"],\n        \"top_p\": cfg[\"top_p\"],\n        \"pad_token_id\": tokenizer.eos_token_id,\n        \"repetition_penalty\": cfg[\"repetition_penalty\"],\n    }\n\n    # Optional top-k\n    if cfg.get(\"top_k\") is not None:\n        generation_kwargs[\"top_k\"] = cfg[\"top_k\"]\n\n    outputs = model.generate(**inputs, **generation_kwargs)\n\n    return tokenizer.decode(\n        outputs[0],\n        skip_special_tokens=cfg[\"skip_special_tokens\"]\n    )\n\n# -------------------------\n# Example Run\n# -------------------------\nif __name__ == \"__main__\":\n    prompt = \"Once upon a time in a futuristic city,\"\n    text = generate(prompt)\n    print(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T23:40:18.098095Z","iopub.execute_input":"2025-12-21T23:40:18.098563Z","iopub.status.idle":"2025-12-21T23:40:18.103765Z","shell.execute_reply.started":"2025-12-21T23:40:18.098540Z","shell.execute_reply":"2025-12-21T23:40:18.103221Z"}},"outputs":[{"name":"stdout","text":"Writing inference/inference.py\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"!python inference/inference.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T23:40:35.104856Z","iopub.execute_input":"2025-12-21T23:40:35.105556Z","iopub.status.idle":"2025-12-21T23:40:48.850008Z","shell.execute_reply.started":"2025-12-21T23:40:35.105529Z","shell.execute_reply":"2025-12-21T23:40:48.849165Z"}},"outputs":[{"name":"stdout","text":"2025-12-21 23:40:41.062552: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766360441.081778     197 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766360441.087250     197 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766360441.104857     197 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766360441.104885     197 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766360441.104890     197 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766360441.104897     197 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nOnce upon a time in a futuristic city, the human mind is faced with the question of whether to follow its own desires , or to follow its own desires . The human mind is faced with the question of whether to act in order to save the people from evil , or to be guided by the desires of others . The human mind is faced with the question of whether to be guided by the desires of others , or to be guided by the desires of the human heart . \n\nIn the book , the story is told of a woman who believes she has the right to change the minds of others , but finds herself trapped in a fantasy world . She has no control over the events in the fantasy world , but is given the opportunity to choose the one who will act for her . She is\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"%%writefile train/train_lm_wandb.py\n\nimport math\nimport yaml\nimport torch\nimport wandb\nfrom torch.utils.data import DataLoader\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    DataCollatorForLanguageModeling,\n    get_scheduler\n)\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n# -------------------------\n# Load config\n# -------------------------\nwith open(\"configs/train_base.yaml\") as f:\n    cfg = yaml.safe_load(f)\n\nwandb.init(\n    project=\"llm-from-scratch\",\n    config=cfg\n)\n\ntorch.manual_seed(cfg[\"seed\"])\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# -------------------------\n# Tokenizer & Model\n# -------------------------\ntokenizer = AutoTokenizer.from_pretrained(cfg[\"model_name\"])\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(cfg[\"model_name\"])\nmodel.to(device)\n\n# -------------------------\n# Dataset\n# -------------------------\ndataset = load_dataset(\n    cfg[\"dataset_name\"],\n    cfg[\"dataset_config\"]\n)\n\ndef tokenize_fn(examples):\n    tokens = tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        max_length=cfg[\"max_length\"],\n        padding=False,\n    )\n\n    # FILTER EMPTY SEQUENCES\n    input_ids = []\n    for ids in tokens[\"input_ids\"]:\n        if len(ids) > 0:\n            input_ids.append(ids)\n\n    return {\"input_ids\": input_ids}\n\ntokenized = dataset.map(\n    tokenize_fn,\n    batched=True,\n    remove_columns=[\"text\"]\n)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\ntrain_loader = DataLoader(\n    tokenized[\"train\"],\n    batch_size=cfg[\"batch_size\"],\n    shuffle=True,\n    collate_fn=data_collator\n)\n\n# -------------------------\n# Optimizer & Scheduler\n# -------------------------\ncfg[\"learning_rate\"] = float(cfg[\"learning_rate\"])\ncfg[\"weight_decay\"] = float(cfg[\"weight_decay\"])\ncfg[\"warmup_ratio\"] = float(cfg.get(\"warmup_ratio\", 0.0))\ncfg[\"max_grad_norm\"] = float(cfg.get(\"max_grad_norm\", 1.0))\n\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=cfg[\"learning_rate\"],\n    weight_decay=cfg[\"weight_decay\"]\n)\n\nnum_training_steps = cfg[\"num_epochs\"] * len(train_loader)\nnum_warmup_steps = int(cfg[\"warmup_ratio\"] * num_training_steps)\n\nlr_scheduler = get_scheduler(\n    name=\"cosine\",\n    optimizer=optimizer,\n    num_warmup_steps=num_warmup_steps,\n    num_training_steps=num_training_steps\n)\n\ndef sanity_check(batch):\n    assert batch[\"input_ids\"].dim() == 2\n    assert batch[\"input_ids\"].size(1) > 0\n    \n# -------------------------\n# Training Loop\n# -------------------------\nmodel.train()\nglobal_step = 0\n\nprogress = tqdm(range(num_training_steps))\n\nfor epoch in range(cfg[\"num_epochs\"]):\n    for batch in train_loader:\n        sanity_check(batch)\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        outputs = model(**batch)\n        loss = outputs.loss\n\n        loss.backward()\n\n        grad_norm = torch.nn.utils.clip_grad_norm_(\n            model.parameters(),\n            cfg[\"max_grad_norm\"]\n        )\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\n        lr = lr_scheduler.get_last_lr()[0]\n\n        wandb.log({\n            \"train/loss\": loss.item(),\n            \"train/perplexity\": math.exp(loss.item()),\n            \"train/lr\": lr,\n            \"train/grad_norm\": grad_norm,\n            \"train/step\": global_step\n        })\n\n        global_step += 1\n        progress.update(1)\n        progress.set_postfix(loss=loss.item(), lr=lr)\n\n# -------------------------\n# Save\n# -------------------------\nmodel.save_pretrained(cfg[\"output_dir\"])\ntokenizer.save_pretrained(cfg[\"output_dir\"])\n\nwandb.finish()\ntorch.cuda.max_memory_allocated()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T21:16:45.513272Z","iopub.execute_input":"2025-12-21T21:16:45.513621Z","iopub.status.idle":"2025-12-21T21:16:45.522469Z","shell.execute_reply.started":"2025-12-21T21:16:45.513590Z","shell.execute_reply":"2025-12-21T21:16:45.521366Z"}},"outputs":[{"name":"stdout","text":"Writing train/train_lm_wandb.py\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!python train/train_lm_wandb.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile train/train_lm_ddp.py\n\nimport os\nimport math\nimport yaml\nimport torch\nimport torch.distributed as dist\nimport wandb\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data import DataLoader, DistributedSampler\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    DataCollatorForLanguageModeling,\n    get_scheduler\n)\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n# -------------------------\n# DDP setup\n# -------------------------\ndef setup_ddp():\n    dist.init_process_group(backend=\"gloo\")\n    rank = dist.get_rank()\n    world_size = dist.get_world_size()\n    return rank, world_size\n\ndef cleanup_ddp():\n    dist.destroy_process_group()\n\nrank, world_size = setup_ddp()\n\n# -------------------------\n# Load config\n# -------------------------\nwith open(\"configs/train_base.yaml\") as f:\n    cfg = yaml.safe_load(f)\n\ntorch.manual_seed(cfg[\"seed\"])\n\ndevice = torch.device(\"cpu\")  # CPU-safe DDP\n\n# -------------------------\n# W&B init (ONLY rank 0)\n# -------------------------\nif rank == 0:\n    wandb.init(\n        project=\"llm-from-scratch\",\n        name=cfg.get(\"wandb_run_name\", \"ddp-cpu-debug\"),\n        config=cfg\n    )\n\n# -------------------------\n# Tokenizer & Model\n# -------------------------\ntokenizer = AutoTokenizer.from_pretrained(cfg[\"model_name\"])\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(cfg[\"model_name\"])\nmodel.to(device)\n\nmodel = DDP(model)\n\n# -------------------------\n# Dataset\n# -------------------------\ndataset = load_dataset(\n    cfg[\"dataset_name\"],\n    cfg[\"dataset_config\"]\n)\n\ndef tokenize_fn(examples):\n    tokens = tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        max_length=cfg[\"max_length\"],\n        padding=False,\n    )\n\n    input_ids = [ids for ids in tokens[\"input_ids\"] if len(ids) > 0]\n    return {\"input_ids\": input_ids}\n\ntokenized = dataset.map(\n    tokenize_fn,\n    batched=True,\n    remove_columns=[\"text\"]\n)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\nsampler = DistributedSampler(\n    tokenized[\"train\"],\n    num_replicas=world_size,\n    rank=rank,\n    shuffle=True\n)\n\ntrain_loader = DataLoader(\n    tokenized[\"train\"],\n    batch_size=cfg[\"batch_size\"],\n    sampler=sampler,\n    collate_fn=data_collator\n)\n\n# -------------------------\n# Optimizer & Scheduler\n# -------------------------\ncfg[\"learning_rate\"] = float(cfg[\"learning_rate\"])\ncfg[\"weight_decay\"] = float(cfg[\"weight_decay\"])\n\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=cfg[\"learning_rate\"],\n    weight_decay=cfg[\"weight_decay\"]\n)\n\nnum_training_steps = cfg[\"num_epochs\"] * len(train_loader)\n\nlr_scheduler = get_scheduler(\n    name=\"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps\n)\n\n# -------------------------\n# Training Loop\n# -------------------------\ndef sanity_check(batch):\n    assert batch[\"input_ids\"].dim() == 2\n    assert batch[\"input_ids\"].size(1) > 0\n\nmodel.train()\nglobal_step = 0\n\nfor epoch in range(cfg[\"num_epochs\"]):\n    sampler.set_epoch(epoch)\n\n    for batch in tqdm(train_loader, disable=(rank != 0)):\n        sanity_check(batch)\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        outputs = model(**batch)\n        loss = outputs.loss\n\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\n        if rank == 0:\n            wandb.log({\n                \"train/loss\": loss.item(),\n                \"train/lr\": lr_scheduler.get_last_lr()[0],\n                \"epoch\": epoch,\n                \"step\": global_step\n            })\n\n        global_step += 1\n\n    if rank == 0:\n        print(f\"Epoch {epoch} Loss: {loss.item():.4f}\")\n\n# -------------------------\n# Save only on rank 0\n# -------------------------\nif rank == 0:\n    model.module.save_pretrained(cfg[\"output_dir\"])\n    tokenizer.save_pretrained(cfg[\"output_dir\"])\n    wandb.finish()\n\ncleanup_ddp()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T21:16:54.170388Z","iopub.execute_input":"2025-12-21T21:16:54.170844Z","iopub.status.idle":"2025-12-21T21:16:54.178948Z","shell.execute_reply.started":"2025-12-21T21:16:54.170706Z","shell.execute_reply":"2025-12-21T21:16:54.177561Z"}},"outputs":[{"name":"stdout","text":"Writing train/train_lm_ddp.py\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!torchrun --nproc_per_node=2 train/train_lm_ddp.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T21:17:11.260684Z","iopub.execute_input":"2025-12-21T21:17:11.261036Z","execution_failed":"2025-12-21T23:07:23.588Z"}},"outputs":[{"name":"stdout","text":"W1221 21:17:17.453000 101 torch/distributed/run.py:774] \nW1221 21:17:17.453000 101 torch/distributed/run.py:774] *****************************************\nW1221 21:17:17.453000 101 torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW1221 21:17:17.453000 101 torch/distributed/run.py:774] *****************************************\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n2025-12-21 21:17:39.102115: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-12-21 21:17:39.102139: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766351859.391025     106 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766351859.390960     107 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766351859.484111     106 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nE0000 00:00:1766351859.484138     107 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766351860.237299     107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766351860.237376     107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766351860.237381     107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766351860.237386     107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766351860.237332     106 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766351860.237402     106 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766351860.237413     106 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766351860.237421     106 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrahulkrish28\u001b[0m (\u001b[33mrahulkrish28-california-state-university-fullerton\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\ntokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26.0/26.0 [00:00<00:00, 126kB/s]\nconfig.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 665/665 [00:00<00:00, 2.12MB/s]\nvocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.04M/1.04M [00:00<00:00, 4.50MB/s]\nmerges.txt:   0%|                                    | 0.00/456k [00:00<?, ?B/s]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m setting up run rpzh2dwq (0.1s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m setting up run rpzh2dwq (0.1s)\nmerges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00<00:00, 3.69MB/s]\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20251221_211800-rpzh2dwq\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mddp-cpu-debug\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rahulkrish28-california-state-university-fullerton/llm-from-scratch\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/rahulkrish28-california-state-university-fullerton/llm-from-scratch/runs/rpzh2dwq\u001b[0m\ntokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.36M/1.36M [00:00<00:00, 5.45MB/s]\nmodel.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 548M/548M [00:02<00:00, 232MB/s]\ngeneration_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 124/124 [00:00<00:00, 738kB/s]\nREADME.md: 10.5kB [00:00, 10.2MB/s]\nwikitext-2-raw-v1/test-00000-of-00001.pa(â€¦): 100%|â–ˆ| 733k/733k [00:00<00:00, 942\nwikitext-2-raw-v1/train-00000-of-00001.p(â€¦): 100%|â–ˆ| 6.36M/6.36M [00:00<00:00, 6\nwikitext-2-raw-v1/validation-00000-of-00(â€¦): 100%|â–ˆ| 657k/657k [00:00<00:00, 1.2\nGenerating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4358/4358 [00:00<00:00, 57678.31 examples/s]\nGenerating train split: 100%|â–ˆâ–ˆ| 36718/36718 [00:00<00:00, 391095.72 examples/s]\nGenerating validation split: 100%|â–ˆ| 3760/3760 [00:00<00:00, 434032.83 examples/\nMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4358/4358 [00:01<00:00, 3684.42 examples/s]\nMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4358/4358 [00:01<00:00, 3309.53 examples/s]\nMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36718/36718 [00:07<00:00, 4688.97 examples/s]\nMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36718/36718 [00:07<00:00, 4672.57 examples/s]\nMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3760/3760 [00:00<00:00, 4619.08 examples/s]\nMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3760/3760 [00:00<00:00, 4913.73 examples/s]\n`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 821/5942 [1:48:48<11:18:31,  7.95s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"%%writefile train/train_lm_amp_ckpt.py\n\nimport math\nimport yaml\nimport torch\nimport wandb\n\nfrom torch.utils.data import DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    DataCollatorForLanguageModeling,\n    get_scheduler\n)\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n# -------------------------\n# Load config\n# -------------------------\nwith open(\"configs/train_base.yaml\") as f:\n    cfg = yaml.safe_load(f)\n\ntorch.manual_seed(cfg[\"seed\"])\ndevice = torch.device(\"cuda\")\n\n# -------------------------\n# Initialize W&B\n# -------------------------\nwandb.init(\n    project=\"llm-from-scratch\",\n    config=cfg,\n    name=\"train_amp_ckpt\"\n)\n\n# -------------------------\n# Tokenizer & Model\n# -------------------------\ntokenizer = AutoTokenizer.from_pretrained(cfg[\"model_name\"])\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(cfg[\"model_name\"])\nmodel.gradient_checkpointing_enable()  # ðŸ”¥ Gradient checkpointing\nmodel.to(device)\n\n# -------------------------\n# Dataset\n# -------------------------\ndataset = load_dataset(\n    cfg[\"dataset_name\"],\n    cfg[\"dataset_config\"]\n)\n\ndef tokenize_fn(examples):\n    tokens = tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        max_length=cfg[\"max_length\"],\n        padding=False,\n    )\n    input_ids = [ids for ids in tokens[\"input_ids\"] if len(ids) > 0]\n    return {\"input_ids\": input_ids}\n\ntokenized = dataset.map(\n    tokenize_fn,\n    batched=True,\n    remove_columns=[\"text\"]\n)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\ntrain_loader = DataLoader(\n    tokenized[\"train\"],\n    batch_size=cfg[\"batch_size\"],\n    shuffle=True,\n    collate_fn=data_collator\n)\n\n# -------------------------\n# Optimizer & Scheduler\n# -------------------------\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=float(cfg[\"learning_rate\"]),\n    weight_decay=float(cfg[\"weight_decay\"])\n)\n\nnum_training_steps = cfg[\"num_epochs\"] * len(train_loader)\nlr_scheduler = get_scheduler(\n    name=\"cosine\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps\n)\n\nscaler = GradScaler()  # ðŸ”¥ AMP scaler\n\n# -------------------------\n# Training Loop\n# -------------------------\ndef sanity_check(batch):\n    assert batch[\"input_ids\"].dim() == 2\n    assert batch[\"input_ids\"].size(1) > 0\n\nmodel.train()\nglobal_step = 0\n\nfor epoch in range(cfg[\"num_epochs\"]):\n    for batch in tqdm(train_loader):\n        sanity_check(batch)\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        with autocast():  # ðŸ”¥ AMP\n            outputs = model(**batch)\n            loss = outputs.loss\n\n        # Backprop with AMP\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        optimizer.zero_grad()\n        lr_scheduler.step()\n\n        # Log metrics to W&B\n        wandb.log({\n            \"train/loss\": loss.item(),\n            \"train/perplexity\": math.exp(loss.item()),\n            \"train/lr\": lr_scheduler.get_last_lr()[0],\n            \"train/global_step\": global_step\n        })\n        global_step += 1\n\n    # Epoch-level logging\n    print(f\"Epoch {epoch} Loss: {loss.item():.4f} | Perplexity: {math.exp(loss.item()):.2f}\")\n    wandb.log({\n        \"epoch/loss\": loss.item(),\n        \"epoch/perplexity\": math.exp(loss.item()),\n        \"epoch\": epoch\n    })\n\n# -------------------------\n# Save\n# -------------------------\nmodel.save_pretrained(cfg[\"output_dir\"])\ntokenizer.save_pretrained(cfg[\"output_dir\"])\n\nwandb.finish()\ntorch.cuda.max_memory_allocated()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T21:17:08.428368Z","iopub.execute_input":"2025-12-21T21:17:08.428731Z","iopub.status.idle":"2025-12-21T21:17:08.438293Z","shell.execute_reply.started":"2025-12-21T21:17:08.428700Z","shell.execute_reply":"2025-12-21T21:17:08.437264Z"}},"outputs":[{"name":"stdout","text":"Writing train/train_lm_amp_ckpt.py\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"%%writefile train/train_lm_lora.py\n\nimport time\nimport math\nimport yaml\nimport torch\nimport wandb\nfrom torch.utils.data import DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    DataCollatorForLanguageModeling,\n    get_scheduler\n)\n\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n# PEFT LoRA\nfrom peft import LoraConfig, get_peft_model, TaskType\n\n# -------------------------\n# Load config\n# -------------------------\nwith open(\"configs/train_base.yaml\") as f:\n    cfg = yaml.safe_load(f)\n\ntorch.manual_seed(cfg[\"seed\"])\ndevice = torch.device(\"cuda\")\n\n# -------------------------\n# W&B init\n# -------------------------\nwandb.init(project=\"llm-from-scratch\", config=cfg, name=\"LoRA-Training\")\n\n# -------------------------\n# Tokenizer & Model\n# -------------------------\ntokenizer = AutoTokenizer.from_pretrained(cfg[\"model_name\"])\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(cfg[\"model_name\"])\nmodel.gradient_checkpointing_enable()  # gradient checkpointing\nmodel.to(device)\n\n# -------------------------\n# Freeze base model & Add LoRA\n# -------------------------\nfor param in model.parameters():\n    param.requires_grad = False\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()  # check trainable params\n\n# -------------------------\n# Dataset\n# -------------------------\ndataset = load_dataset(\n    cfg[\"dataset_name\"],\n    cfg[\"dataset_config\"]\n)\n\ndef tokenize_fn(examples):\n    tokens = tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        max_length=cfg[\"max_length\"],\n        padding=False,\n    )\n    input_ids = [ids for ids in tokens[\"input_ids\"] if len(ids) > 0]\n    return {\"input_ids\": input_ids}\n\ntokenized = dataset.map(\n    tokenize_fn,\n    batched=True,\n    remove_columns=[\"text\"]\n)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\ntrain_loader = DataLoader(\n    tokenized[\"train\"],\n    batch_size=cfg[\"batch_size\"],\n    shuffle=True,\n    collate_fn=data_collator\n)\n\n# -------------------------\n# Optimizer & Scheduler\n# -------------------------\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=float(cfg[\"learning_rate\"]),\n    weight_decay=float(cfg[\"weight_decay\"])\n)\n\nnum_training_steps = cfg[\"num_epochs\"] * len(train_loader)\nlr_scheduler = get_scheduler(\n    name=\"cosine\",\n    optimizer=optimizer,\n    num_warmup_steps=int(0.1 * num_training_steps),\n    num_training_steps=num_training_steps\n)\n\nscaler = GradScaler()  # AMP\n\n# -------------------------\n# Training Loop + W&B logging\n# -------------------------\ndef sanity_check(batch):\n    assert batch[\"input_ids\"].dim() == 2\n    assert batch[\"input_ids\"].size(1) > 0\n    \nmodel.train()\nglobal_step = 0\nstep_times = []\n\nfor epoch in range(cfg[\"num_epochs\"]):\n    for batch in tqdm(train_loader):\n        sanity_check(batch)\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        start = time.time()\n        with autocast():\n            outputs = model(**batch)\n            loss = outputs.loss\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        optimizer.zero_grad()\n        lr_scheduler.step()\n        end = time.time()\n\n        step_time = end - start\n        step_times.append(step_time)\n\n        # Log GPU memory\n        max_mem = torch.cuda.max_memory_allocated() / 1024**2  # MB\n        lr = lr_scheduler.get_last_lr()[0]\n\n        # W&B logging\n        wandb.log({\n            \"train/loss\": loss.item(),\n            \"train/perplexity\": math.exp(loss.item()),\n            \"train/lr\": lr,\n            \"train/max_memory_MB\": max_mem,\n            \"train/step_time_s\": step_time,\n            \"train/global_step\": global_step\n        })\n\n        global_step += 1\n\n        tqdm.write(\n            f\"Epoch {epoch} | Step {global_step} | Loss: {loss.item():.4f} | \"\n            f\"Mem: {max_mem:.1f}MB | Step Time: {step_time:.2f}s\"\n        )\n\n# -------------------------\n# Save LoRA adapters only\n# -------------------------\nmodel.save_pretrained(cfg[\"output_dir\"] + \"_lora\")\ntokenizer.save_pretrained(cfg[\"output_dir\"])\n\nwandb.finish()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}