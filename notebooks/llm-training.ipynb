{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Create Folders","metadata":{}},{"cell_type":"code","source":"import os\nos.makedirs('configs', exist_ok=True)\nos.makedirs('train', exist_ok=True)\nos.makedirs('inference', exist_ok=True)\nos.makedirs('data', exist_ok=True)\nos.makedirs('rlhf', exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T08:50:15.214787Z","iopub.execute_input":"2026-01-01T08:50:15.214957Z","iopub.status.idle":"2026-01-01T08:50:15.222294Z","shell.execute_reply.started":"2026-01-01T08:50:15.214938Z","shell.execute_reply":"2026-01-01T08:50:15.221629Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T08:50:19.077733Z","iopub.execute_input":"2026-01-01T08:50:19.078049Z","iopub.status.idle":"2026-01-01T08:50:23.022020Z","shell.execute_reply.started":"2026-01-01T08:50:19.078024Z","shell.execute_reply":"2026-01-01T08:50:23.021364Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.22.2)\nRequirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.1)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.1)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.12.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.5)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.42.1)\nRequirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.11.12)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\n\n# Load W&B key from Kaggle Secrets\nos.environ[\"WANDB_API_KEY\"] = UserSecretsClient().get_secret(\"WANDB_API_KEY\")\nos.environ[\"WANDB_PROJECT\"] = \"llm-from-scratch\"\nos.environ[\"WANDB_ENTITY\"] = \"rahulkrish28-california-state-university-fullerton\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T08:50:32.885506Z","iopub.execute_input":"2026-01-01T08:50:32.886079Z","iopub.status.idle":"2026-01-01T08:50:33.012468Z","shell.execute_reply.started":"2026-01-01T08:50:32.886045Z","shell.execute_reply":"2026-01-01T08:50:33.011926Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"%%writefile configs/train_base.yaml\n\nmodel_name: gpt2\ndataset_name: wikitext\ndataset_config: wikitext-2-raw-v1\n\nmax_length: 512\nbatch_size: 2\nnum_epochs: 1\n\nlearning_rate: 5e-5\nweight_decay: 0.01\nwarmup_ratio: 0.1\nmax_grad_norm: 1.0\n\noutput_dir: outputs/train_lm\noutput_dir2: outputs/train_lm_wandb\noutput_dir3: outputs/train_lm_ddp\noutput_dir4: outputs/train_lm_amp_ckpt\noutput_dir5: outputs/train_lm_lora\nseed: 42","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T09:44:29.574135Z","iopub.execute_input":"2026-01-01T09:44:29.574487Z","iopub.status.idle":"2026-01-01T09:44:29.579900Z","shell.execute_reply.started":"2026-01-01T09:44:29.574450Z","shell.execute_reply":"2026-01-01T09:44:29.579175Z"}},"outputs":[{"name":"stdout","text":"Overwriting configs/train_base.yaml\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"%%writefile configs/inference.yaml\n# =========================\n# Inference Configuration\n# =========================\n\n# -------------------------\n# Model\n# -------------------------\nmodel_dir: \"outputs/train_lm\"   # same as cfg[\"output_dir\"] during training\ndevice: \"auto\"                  # auto | cuda | cpu\n\n# -------------------------\n# Tokenization\n# -------------------------\nmax_input_length: 512           # truncate prompt if longer than this\n\n# -------------------------\n# Generation Parameters\n# -------------------------\nmax_new_tokens: 150             # number of tokens to generate\ndo_sample: true                 # sampling vs greedy decoding\n\ntemperature: 0.8                # randomness (1.0 = neutral)\ntop_p: 0.9                      # nucleus sampling\ntop_k: 50                       # optional, can be null\nrepetition_penalty: 1.0         # >1.0 reduces repetition\n\n# -------------------------\n# Special Tokens\n# -------------------------\npad_token: \"eos\"                # use EOS as PAD\nskip_special_tokens: true\n\n# -------------------------\n# Runtime\n# -------------------------\nuse_fp16: false                 # set true if GPU supports fp16\nbatch_size: 1                   # for batch inference","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T08:50:42.908529Z","iopub.execute_input":"2026-01-01T08:50:42.908813Z","iopub.status.idle":"2026-01-01T08:50:42.913850Z","shell.execute_reply.started":"2026-01-01T08:50:42.908789Z","shell.execute_reply":"2026-01-01T08:50:42.913217Z"}},"outputs":[{"name":"stdout","text":"Writing configs/inference.yaml\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"%%writefile train/train_lm.py\n\nimport math\nimport yaml\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    DataCollatorForLanguageModeling,\n    get_scheduler\n)\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n# -------------------------\n# Load config\n# -------------------------\nwith open(\"configs/train_base.yaml\") as f:\n    cfg = yaml.safe_load(f)\n\ntorch.manual_seed(cfg[\"seed\"])\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# -------------------------\n# Tokenizer & Model\n# -------------------------\ntokenizer = AutoTokenizer.from_pretrained(cfg[\"model_name\"])\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(cfg[\"model_name\"])\nmodel.to(device)\n\n# -------------------------\n# Dataset\n# -------------------------\ndataset = load_dataset(\n    cfg[\"dataset_name\"],\n    cfg[\"dataset_config\"]\n)\n\ndef tokenize_fn(examples):\n    tokens = tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        max_length=cfg[\"max_length\"],\n        padding=False,\n    )\n\n    # FILTER EMPTY SEQUENCES\n    input_ids = []\n    for ids in tokens[\"input_ids\"]:\n        if len(ids) > 0:\n            input_ids.append(ids)\n\n    return {\"input_ids\": input_ids}\n\ntokenized = dataset.map(\n    tokenize_fn,\n    batched=True,\n    remove_columns=[\"text\"]\n)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\ntrain_loader = DataLoader(\n    tokenized[\"train\"],\n    batch_size=cfg[\"batch_size\"],\n    shuffle=True,\n    collate_fn=data_collator\n)\n\n# -------------------------\n# Optimizer\n# -------------------------\n# ---- Defensive casting ----\ncfg[\"learning_rate\"] = float(cfg[\"learning_rate\"])\ncfg[\"weight_decay\"] = float(cfg[\"weight_decay\"])\n\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=cfg[\"learning_rate\"],\n    weight_decay=cfg[\"weight_decay\"]\n)\n\nnum_training_steps = cfg[\"num_epochs\"] * len(train_loader)\nlr_scheduler = get_scheduler(\n    name=\"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps\n)\n\ndef sanity_check(batch):\n    assert batch[\"input_ids\"].dim() == 2\n    assert batch[\"input_ids\"].size(1) > 0\n    \n# -------------------------\n# Training Loop\n# -------------------------\nmodel.train()\nprogress = tqdm(range(num_training_steps))\n\nfor epoch in range(cfg[\"num_epochs\"]):\n    for batch in train_loader:\n        sanity_check(batch)\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        outputs = model(**batch)\n        loss = outputs.loss\n\n        loss.backward()\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\n        progress.update(1)\n        progress.set_postfix(loss=loss.item())\n\n# -------------------------\n# Perplexity\n# -------------------------\nppl = math.exp(loss.item())\nprint(f\"Final Perplexity: {ppl:.2f}\")\n\nmodel.save_pretrained(cfg[\"output_dir\"])\ntokenizer.save_pretrained(cfg[\"output_dir\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T08:50:47.711820Z","iopub.execute_input":"2026-01-01T08:50:47.712429Z","iopub.status.idle":"2026-01-01T08:50:47.717883Z","shell.execute_reply.started":"2026-01-01T08:50:47.712403Z","shell.execute_reply":"2026-01-01T08:50:47.717215Z"}},"outputs":[{"name":"stdout","text":"Writing train/train_lm.py\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!python train/train_lm.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T08:51:01.830089Z","iopub.execute_input":"2026-01-01T08:51:01.830753Z","iopub.status.idle":"2026-01-01T09:18:43.200808Z","shell.execute_reply.started":"2026-01-01T08:51:01.830720Z","shell.execute_reply":"2026-01-01T09:18:43.199941Z"}},"outputs":[{"name":"stdout","text":"2026-01-01 08:51:14.657984: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767257474.854291     106 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767257474.909789     106 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767257475.373935     106 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767257475.373979     106 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767257475.373983     106 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767257475.373987     106 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\ntokenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26.0/26.0 [00:00<00:00, 184kB/s]\nconfig.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [00:00<00:00, 5.92MB/s]\nvocab.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.04M/1.04M [00:00<00:00, 20.1MB/s]\nmerges.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 456k/456k [00:00<00:00, 33.4MB/s]\ntokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.36M/1.36M [00:00<00:00, 24.0MB/s]\nmodel.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 548M/548M [00:01<00:00, 309MB/s]\ngeneration_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124/124 [00:00<00:00, 1.09MB/s]\nREADME.md: 10.5kB [00:00, 27.1MB/s]\nwikitext-2-raw-v1/test-00000-of-00001.pa(‚Ä¶): 100%|‚ñà| 733k/733k [00:00<00:00, 3.3\nwikitext-2-raw-v1/train-00000-of-00001.p(‚Ä¶): 100%|‚ñà| 6.36M/6.36M [00:00<00:00, 1\nwikitext-2-raw-v1/validation-00000-of-00(‚Ä¶): 100%|‚ñà| 657k/657k [00:00<00:00, 2.5\nGenerating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4358/4358 [00:00<00:00, 90288.30 examples/s]\nGenerating train split: 100%|‚ñà‚ñà| 36718/36718 [00:00<00:00, 772283.48 examples/s]\nGenerating validation split: 100%|‚ñà| 3760/3760 [00:00<00:00, 563758.60 examples/\nMap: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4358/4358 [00:00<00:00, 11465.46 examples/s]\nMap: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36718/36718 [00:03<00:00, 10667.93 examples/s]\nMap: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3760/3760 [00:00<00:00, 6849.07 examples/s]\n  0%|                                                 | 0/11884 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11884/11884 [26:59<00:00,  9.64it/s, loss=2.66]Final Perplexity: 14.32\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11884/11884 [27:00<00:00,  7.33it/s, loss=2.66]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"%%writefile inference/inference.py\n\nimport yaml\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# -------------------------\n# Load Inference Config\n# -------------------------\nwith open(\"configs/inference.yaml\") as f:\n    cfg = yaml.safe_load(f)\n\n# -------------------------\n# Device\n# -------------------------\nif cfg[\"device\"] == \"auto\":\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nelse:\n    device = cfg[\"device\"]\n\n# -------------------------\n# Tokenizer & Model\n# -------------------------\ntokenizer = AutoTokenizer.from_pretrained(cfg[\"model_dir\"])\n\n# Pad token handling\nif cfg[\"pad_token\"] == \"eos\":\n    tokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(cfg[\"model_dir\"])\n\nif cfg.get(\"use_fp16\", False) and device == \"cuda\":\n    model = model.half()\n\nmodel.to(device)\nmodel.eval()\n\n# -------------------------\n# Generation Function\n# -------------------------\n@torch.no_grad()\ndef generate(prompt: str):\n    inputs = tokenizer(\n        prompt,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=cfg[\"max_input_length\"],\n    ).to(device)\n\n    generation_kwargs = {\n        \"max_new_tokens\": cfg[\"max_new_tokens\"],\n        \"do_sample\": cfg[\"do_sample\"],\n        \"temperature\": cfg[\"temperature\"],\n        \"top_p\": cfg[\"top_p\"],\n        \"pad_token_id\": tokenizer.eos_token_id,\n        \"repetition_penalty\": cfg[\"repetition_penalty\"],\n    }\n\n    # Optional top-k\n    if cfg.get(\"top_k\") is not None:\n        generation_kwargs[\"top_k\"] = cfg[\"top_k\"]\n\n    outputs = model.generate(**inputs, **generation_kwargs)\n\n    return tokenizer.decode(\n        outputs[0],\n        skip_special_tokens=cfg[\"skip_special_tokens\"]\n    )\n\n# -------------------------\n# Example Run\n# -------------------------\nif __name__ == \"__main__\":\n    prompt = \"Once upon a time in a futuristic city,\"\n    text = generate(prompt)\n    print(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T09:19:02.383749Z","iopub.execute_input":"2026-01-01T09:19:02.384373Z","iopub.status.idle":"2026-01-01T09:19:02.389828Z","shell.execute_reply.started":"2026-01-01T09:19:02.384342Z","shell.execute_reply":"2026-01-01T09:19:02.389107Z"}},"outputs":[{"name":"stdout","text":"Writing inference/inference.py\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!python inference/inference.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T09:19:07.412577Z","iopub.execute_input":"2026-01-01T09:19:07.412853Z","iopub.status.idle":"2026-01-01T09:19:20.947075Z","shell.execute_reply.started":"2026-01-01T09:19:07.412829Z","shell.execute_reply":"2026-01-01T09:19:20.946442Z"}},"outputs":[{"name":"stdout","text":"2026-01-01 09:19:13.208031: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767259153.224757     216 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767259153.229777     216 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767259153.245738     216 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767259153.245765     216 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767259153.245769     216 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767259153.245772     216 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nOnce upon a time in a futuristic city, the world has begun to re @-@ discover its history . \n\nBlessings to the city , the city 's inhabitants and its inhabitants must now return to the city to help in the fight against the darkness . \n\nCaught between the darkness and the city 's inhabitants , the traveler finds his way back to the city . \n\nCalled The Watcher , the traveler begins to discover the world of the Watchers . \n\nA member of the Watchers who are not part of the city 's ruling class , the Watcher returns to the city and fights against the Watchers . \n\nA Watcher who is not part of the city 's ruling class , the Watcher returns to\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"%%writefile train/train_lm_wandb.py\n\nimport math\nimport yaml\nimport torch\nimport wandb\nfrom torch.utils.data import DataLoader\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    DataCollatorForLanguageModeling,\n    get_scheduler\n)\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n# -------------------------\n# Load config\n# -------------------------\nwith open(\"configs/train_base.yaml\") as f:\n    cfg = yaml.safe_load(f)\n\nwandb.init(\n    project=\"llm-from-scratch\",\n    config=cfg\n)\n\ntorch.manual_seed(cfg[\"seed\"])\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# -------------------------\n# Tokenizer & Model\n# -------------------------\ntokenizer = AutoTokenizer.from_pretrained(cfg[\"model_name\"])\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(cfg[\"model_name\"])\nmodel.to(device)\n\n# -------------------------\n# Dataset\n# -------------------------\ndataset = load_dataset(\n    cfg[\"dataset_name\"],\n    cfg[\"dataset_config\"]\n)\n\ndef tokenize_fn(examples):\n    tokens = tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        max_length=cfg[\"max_length\"],\n        padding=False,\n    )\n\n    # FILTER EMPTY SEQUENCES\n    input_ids = []\n    for ids in tokens[\"input_ids\"]:\n        if len(ids) > 0:\n            input_ids.append(ids)\n\n    return {\"input_ids\": input_ids}\n\ntokenized = dataset.map(\n    tokenize_fn,\n    batched=True,\n    remove_columns=[\"text\"]\n)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\ntrain_loader = DataLoader(\n    tokenized[\"train\"],\n    batch_size=cfg[\"batch_size\"],\n    shuffle=True,\n    collate_fn=data_collator\n)\n\n# -------------------------\n# Optimizer & Scheduler\n# -------------------------\ncfg[\"learning_rate\"] = float(cfg[\"learning_rate\"])\ncfg[\"weight_decay\"] = float(cfg[\"weight_decay\"])\ncfg[\"warmup_ratio\"] = float(cfg.get(\"warmup_ratio\", 0.0))\ncfg[\"max_grad_norm\"] = float(cfg.get(\"max_grad_norm\", 1.0))\n\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=cfg[\"learning_rate\"],\n    weight_decay=cfg[\"weight_decay\"]\n)\n\nnum_training_steps = cfg[\"num_epochs\"] * len(train_loader)\nnum_warmup_steps = int(cfg[\"warmup_ratio\"] * num_training_steps)\n\nlr_scheduler = get_scheduler(\n    name=\"cosine\",\n    optimizer=optimizer,\n    num_warmup_steps=num_warmup_steps,\n    num_training_steps=num_training_steps\n)\n\ndef sanity_check(batch):\n    assert batch[\"input_ids\"].dim() == 2\n    assert batch[\"input_ids\"].size(1) > 0\n    \n# -------------------------\n# Training Loop\n# -------------------------\nmodel.train()\nglobal_step = 0\n\nprogress = tqdm(range(num_training_steps))\n\nfor epoch in range(cfg[\"num_epochs\"]):\n    for batch in train_loader:\n        sanity_check(batch)\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        outputs = model(**batch)\n        loss = outputs.loss\n\n        loss.backward()\n\n        grad_norm = torch.nn.utils.clip_grad_norm_(\n            model.parameters(),\n            cfg[\"max_grad_norm\"]\n        )\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\n        lr = lr_scheduler.get_last_lr()[0]\n\n        wandb.log({\n            \"train/loss\": loss.item(),\n            \"train/perplexity\": math.exp(loss.item()),\n            \"train/lr\": lr,\n            \"train/grad_norm\": grad_norm,\n            \"train/step\": global_step\n        })\n\n        global_step += 1\n        progress.update(1)\n        progress.set_postfix(loss=loss.item(), lr=lr)\n\n# -------------------------\n# Save\n# -------------------------\nmodel.save_pretrained(cfg[\"output_dir2\"])\ntokenizer.save_pretrained(cfg[\"output_dir2\"])\n\nwandb.finish()\ntorch.cuda.max_memory_allocated()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T21:16:45.513272Z","iopub.execute_input":"2025-12-21T21:16:45.513621Z","iopub.status.idle":"2025-12-21T21:16:45.522469Z","shell.execute_reply.started":"2025-12-21T21:16:45.513590Z","shell.execute_reply":"2025-12-21T21:16:45.521366Z"}},"outputs":[{"name":"stdout","text":"Writing train/train_lm_wandb.py\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!python train/train_lm_wandb.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile train/train_lm_ddp.py\n\nimport os\nimport math\nimport yaml\nimport torch\nimport torch.distributed as dist\nimport wandb\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data import DataLoader, DistributedSampler\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    DataCollatorForLanguageModeling,\n    get_scheduler\n)\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n# -------------------------\n# DDP setup\n# -------------------------\ndef setup_ddp():\n    dist.init_process_group(backend=\"gloo\")\n    rank = dist.get_rank()\n    world_size = dist.get_world_size()\n    return rank, world_size\n\ndef cleanup_ddp():\n    dist.destroy_process_group()\n\nrank, world_size = setup_ddp()\n\n# -------------------------\n# Load config\n# -------------------------\nwith open(\"configs/train_base.yaml\") as f:\n    cfg = yaml.safe_load(f)\n\ntorch.manual_seed(cfg[\"seed\"])\n\ndevice = torch.device(\"cpu\")  # CPU-safe DDP\n\n# -------------------------\n# W&B init (ONLY rank 0)\n# -------------------------\nif rank == 0:\n    wandb.init(\n        project=\"llm-from-scratch\",\n        name=cfg.get(\"wandb_run_name\", \"ddp-cpu-debug\"),\n        config=cfg\n    )\n\n# -------------------------\n# Tokenizer & Model\n# -------------------------\ntokenizer = AutoTokenizer.from_pretrained(cfg[\"model_name\"])\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(cfg[\"model_name\"])\nmodel.to(device)\n\nmodel = DDP(model)\n\n# -------------------------\n# Dataset\n# -------------------------\ndataset = load_dataset(\n    cfg[\"dataset_name\"],\n    cfg[\"dataset_config\"]\n)\n\ndef tokenize_fn(examples):\n    tokens = tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        max_length=cfg[\"max_length\"],\n        padding=False,\n    )\n\n    input_ids = [ids for ids in tokens[\"input_ids\"] if len(ids) > 0]\n    return {\"input_ids\": input_ids}\n\ntokenized = dataset.map(\n    tokenize_fn,\n    batched=True,\n    remove_columns=[\"text\"]\n)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\nsampler = DistributedSampler(\n    tokenized[\"train\"],\n    num_replicas=world_size,\n    rank=rank,\n    shuffle=True\n)\n\ntrain_loader = DataLoader(\n    tokenized[\"train\"],\n    batch_size=cfg[\"batch_size\"],\n    sampler=sampler,\n    collate_fn=data_collator\n)\n\n# -------------------------\n# Optimizer & Scheduler\n# -------------------------\ncfg[\"learning_rate\"] = float(cfg[\"learning_rate\"])\ncfg[\"weight_decay\"] = float(cfg[\"weight_decay\"])\n\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=cfg[\"learning_rate\"],\n    weight_decay=cfg[\"weight_decay\"]\n)\n\nnum_training_steps = cfg[\"num_epochs\"] * len(train_loader)\n\nlr_scheduler = get_scheduler(\n    name=\"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps\n)\n\n# -------------------------\n# Training Loop\n# -------------------------\ndef sanity_check(batch):\n    assert batch[\"input_ids\"].dim() == 2\n    assert batch[\"input_ids\"].size(1) > 0\n\nmodel.train()\nglobal_step = 0\n\nfor epoch in range(cfg[\"num_epochs\"]):\n    sampler.set_epoch(epoch)\n\n    for batch in tqdm(train_loader, disable=(rank != 0)):\n        sanity_check(batch)\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        outputs = model(**batch)\n        loss = outputs.loss\n\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\n        if rank == 0:\n            wandb.log({\n                \"train/loss\": loss.item(),\n                \"train/lr\": lr_scheduler.get_last_lr()[0],\n                \"epoch\": epoch,\n                \"step\": global_step\n            })\n\n        global_step += 1\n\n    if rank == 0:\n        print(f\"Epoch {epoch} Loss: {loss.item():.4f}\")\n\n# -------------------------\n# Save only on rank 0\n# -------------------------\nif rank == 0:\n    model.module.save_pretrained(cfg[\"output_dir3\"])\n    tokenizer.save_pretrained(cfg[\"output_dir3\"])\n    wandb.finish()\n\ncleanup_ddp()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T21:16:54.170388Z","iopub.execute_input":"2025-12-21T21:16:54.170844Z","iopub.status.idle":"2025-12-21T21:16:54.178948Z","shell.execute_reply.started":"2025-12-21T21:16:54.170706Z","shell.execute_reply":"2025-12-21T21:16:54.177561Z"}},"outputs":[{"name":"stdout","text":"Writing train/train_lm_ddp.py\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!torchrun --nproc_per_node=2 train/train_lm_ddp.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T21:17:11.260684Z","iopub.execute_input":"2025-12-21T21:17:11.261036Z","execution_failed":"2025-12-21T23:07:23.588Z"}},"outputs":[{"name":"stdout","text":"W1221 21:17:17.453000 101 torch/distributed/run.py:774] \nW1221 21:17:17.453000 101 torch/distributed/run.py:774] *****************************************\nW1221 21:17:17.453000 101 torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW1221 21:17:17.453000 101 torch/distributed/run.py:774] *****************************************\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n2025-12-21 21:17:39.102115: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-12-21 21:17:39.102139: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766351859.391025     106 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766351859.390960     107 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766351859.484111     106 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nE0000 00:00:1766351859.484138     107 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766351860.237299     107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766351860.237376     107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766351860.237381     107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766351860.237386     107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766351860.237332     106 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766351860.237402     106 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766351860.237413     106 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766351860.237421     106 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrahulkrish28\u001b[0m (\u001b[33mrahulkrish28-california-state-university-fullerton\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\ntokenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26.0/26.0 [00:00<00:00, 126kB/s]\nconfig.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [00:00<00:00, 2.12MB/s]\nvocab.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.04M/1.04M [00:00<00:00, 4.50MB/s]\nmerges.txt:   0%|                                    | 0.00/456k [00:00<?, ?B/s]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m setting up run rpzh2dwq (0.1s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m setting up run rpzh2dwq (0.1s)\nmerges.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 456k/456k [00:00<00:00, 3.69MB/s]\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20251221_211800-rpzh2dwq\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mddp-cpu-debug\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/rahulkrish28-california-state-university-fullerton/llm-from-scratch\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/rahulkrish28-california-state-university-fullerton/llm-from-scratch/runs/rpzh2dwq\u001b[0m\ntokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.36M/1.36M [00:00<00:00, 5.45MB/s]\nmodel.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 548M/548M [00:02<00:00, 232MB/s]\ngeneration_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124/124 [00:00<00:00, 738kB/s]\nREADME.md: 10.5kB [00:00, 10.2MB/s]\nwikitext-2-raw-v1/test-00000-of-00001.pa(‚Ä¶): 100%|‚ñà| 733k/733k [00:00<00:00, 942\nwikitext-2-raw-v1/train-00000-of-00001.p(‚Ä¶): 100%|‚ñà| 6.36M/6.36M [00:00<00:00, 6\nwikitext-2-raw-v1/validation-00000-of-00(‚Ä¶): 100%|‚ñà| 657k/657k [00:00<00:00, 1.2\nGenerating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4358/4358 [00:00<00:00, 57678.31 examples/s]\nGenerating train split: 100%|‚ñà‚ñà| 36718/36718 [00:00<00:00, 391095.72 examples/s]\nGenerating validation split: 100%|‚ñà| 3760/3760 [00:00<00:00, 434032.83 examples/\nMap: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4358/4358 [00:01<00:00, 3684.42 examples/s]\nMap: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4358/4358 [00:01<00:00, 3309.53 examples/s]\nMap: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36718/36718 [00:07<00:00, 4688.97 examples/s]\nMap: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36718/36718 [00:07<00:00, 4672.57 examples/s]\nMap: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3760/3760 [00:00<00:00, 4619.08 examples/s]\nMap: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3760/3760 [00:00<00:00, 4913.73 examples/s]\n`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n 14%|‚ñà‚ñà‚ñà‚ñà‚ñä                              | 821/5942 [1:48:48<11:18:31,  7.95s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"%%writefile train/train_lm_amp_ckpt.py\n\nimport math\nimport yaml\nimport torch\nimport wandb\n\nfrom torch.utils.data import DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    DataCollatorForLanguageModeling,\n    get_scheduler\n)\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n# -------------------------\n# Load config\n# -------------------------\nwith open(\"configs/train_base.yaml\") as f:\n    cfg = yaml.safe_load(f)\n\ntorch.manual_seed(cfg[\"seed\"])\ndevice = torch.device(\"cuda\")\n\n# -------------------------\n# Initialize W&B\n# -------------------------\nwandb.init(\n    project=\"llm-from-scratch\",\n    config=cfg,\n    name=\"train_amp_ckpt\"\n)\n\n# -------------------------\n# Tokenizer & Model\n# -------------------------\ntokenizer = AutoTokenizer.from_pretrained(cfg[\"model_name\"])\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(cfg[\"model_name\"])\nmodel.gradient_checkpointing_enable()  # üî• Gradient checkpointing\nmodel.to(device)\n\n# -------------------------\n# Dataset\n# -------------------------\ndataset = load_dataset(\n    cfg[\"dataset_name\"],\n    cfg[\"dataset_config\"]\n)\n\ndef tokenize_fn(examples):\n    tokens = tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        max_length=cfg[\"max_length\"],\n        padding=False,\n    )\n    input_ids = [ids for ids in tokens[\"input_ids\"] if len(ids) > 0]\n    return {\"input_ids\": input_ids}\n\ntokenized = dataset.map(\n    tokenize_fn,\n    batched=True,\n    remove_columns=[\"text\"]\n)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\ntrain_loader = DataLoader(\n    tokenized[\"train\"],\n    batch_size=cfg[\"batch_size\"],\n    shuffle=True,\n    collate_fn=data_collator\n)\n\n# -------------------------\n# Optimizer & Scheduler\n# -------------------------\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=float(cfg[\"learning_rate\"]),\n    weight_decay=float(cfg[\"weight_decay\"])\n)\n\nnum_training_steps = cfg[\"num_epochs\"] * len(train_loader)\nlr_scheduler = get_scheduler(\n    name=\"cosine\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps\n)\n\nscaler = GradScaler()  # üî• AMP scaler\n\n# -------------------------\n# Training Loop\n# -------------------------\ndef sanity_check(batch):\n    assert batch[\"input_ids\"].dim() == 2\n    assert batch[\"input_ids\"].size(1) > 0\n\nmodel.train()\nglobal_step = 0\n\nfor epoch in range(cfg[\"num_epochs\"]):\n    for batch in tqdm(train_loader):\n        sanity_check(batch)\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        with autocast():  # üî• AMP\n            outputs = model(**batch)\n            loss = outputs.loss\n\n        # Backprop with AMP\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        optimizer.zero_grad()\n        lr_scheduler.step()\n\n        # Log metrics to W&B\n        wandb.log({\n            \"train/loss\": loss.item(),\n            \"train/perplexity\": math.exp(loss.item()),\n            \"train/lr\": lr_scheduler.get_last_lr()[0],\n            \"train/global_step\": global_step\n        })\n        global_step += 1\n\n    # Epoch-level logging\n    print(f\"Epoch {epoch} Loss: {loss.item():.4f} | Perplexity: {math.exp(loss.item()):.2f}\")\n    wandb.log({\n        \"epoch/loss\": loss.item(),\n        \"epoch/perplexity\": math.exp(loss.item()),\n        \"epoch\": epoch\n    })\n\n# -------------------------\n# Save\n# -------------------------\nmodel.save_pretrained(cfg[\"output_dir4\"])\ntokenizer.save_pretrained(cfg[\"output_dir4\"])\n\nwandb.finish()\ntorch.cuda.max_memory_allocated()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T09:45:00.283229Z","iopub.execute_input":"2026-01-01T09:45:00.283758Z","iopub.status.idle":"2026-01-01T09:45:00.289856Z","shell.execute_reply.started":"2026-01-01T09:45:00.283732Z","shell.execute_reply":"2026-01-01T09:45:00.289222Z"}},"outputs":[{"name":"stdout","text":"Overwriting train/train_lm_amp_ckpt.py\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"!python train/train_lm_amp_ckpt.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T09:45:00.823860Z","iopub.execute_input":"2026-01-01T09:45:00.824355Z","iopub.status.idle":"2026-01-01T10:07:43.568278Z","shell.execute_reply.started":"2026-01-01T09:45:00.824331Z","shell.execute_reply":"2026-01-01T10:07:43.567410Z"}},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n2026-01-01 09:45:07.146887: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767260707.169873     337 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767260707.176751     337 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767260707.196049     337 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767260707.196078     337 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767260707.196083     337 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767260707.196089     337 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrahulkrish28\u001b[0m (\u001b[33mrahulkrish28-california-state-university-fullerton\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m Waiting for wandb.init()...\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20260101_094510-wdj5podz\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtrain_amp_ckpt\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/rahulkrish28-california-state-university-fullerton/llm-from-scratch\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/rahulkrish28-california-state-university-fullerton/llm-from-scratch/runs/wdj5podz\u001b[0m\nMap: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4358/4358 [00:00<00:00, 11655.76 examples/s]\nMap: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36718/36718 [00:03<00:00, 10835.31 examples/s]\nMap: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3760/3760 [00:00<00:00, 11757.52 examples/s]\n/kaggle/working/train/train_lm_amp_ckpt.py:100: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()  # üî• AMP scaler\n  0%|                                                 | 0/11884 [00:00<?, ?it/s]/kaggle/working/train/train_lm_amp_ckpt.py:117: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():  # üî• AMP\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11884/11884 [22:21<00:00,  8.86it/s]\nEpoch 0 Loss: 2.6255 | Perplexity: 13.81\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m updating run metadata (0.0s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m updating run metadata (0.0s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m updating run metadata (0.0s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m updating run metadata (0.0s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m updating run metadata (0.0s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m:             epoch ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:        epoch/loss ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:  epoch/perplexity ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: train/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n\u001b[34m\u001b[1mwandb\u001b[0m:        train/loss ‚ñà‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÜ‚ñà‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñá‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ\n\u001b[34m\u001b[1mwandb\u001b[0m:          train/lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:  train/perplexity ‚ñà‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñÑ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÅ‚ñÑ‚ñÉ‚ñÅ‚ñÖ‚ñÉ‚ñÇ‚ñá‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñÑ‚ñÅ‚ñÉ\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m:             epoch 0\n\u001b[34m\u001b[1mwandb\u001b[0m:        epoch/loss 2.62546\n\u001b[34m\u001b[1mwandb\u001b[0m:  epoch/perplexity 13.81094\n\u001b[34m\u001b[1mwandb\u001b[0m: train/global_step 11883\n\u001b[34m\u001b[1mwandb\u001b[0m:        train/loss 2.62546\n\u001b[34m\u001b[1mwandb\u001b[0m:          train/lr 0\n\u001b[34m\u001b[1mwandb\u001b[0m:  train/perplexity 13.81094\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mtrain_amp_ckpt\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rahulkrish28-california-state-university-fullerton/llm-from-scratch/runs/wdj5podz\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/rahulkrish28-california-state-university-fullerton/llm-from-scratch\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20260101_094510-wdj5podz/logs\u001b[0m\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"%%writefile inference/inference_amp_ckpt.py\n\nimport yaml\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# -------------------------\n# Load Inference Config\n# -------------------------\nwith open(\"configs/inference.yaml\") as f:\n    cfg = yaml.safe_load(f)\n\n# -------------------------\n# Device\n# -------------------------\nif cfg[\"device\"] == \"auto\":\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nelse:\n    device = cfg[\"device\"]\n\n# -------------------------\n# Tokenizer & Model\n# -------------------------\ntokenizer = AutoTokenizer.from_pretrained(\"outputs/train_lm_amp_ckpt\")\n\n# Pad token handling\nif cfg[\"pad_token\"] == \"eos\":\n    tokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(\"outputs/train_lm_amp_ckpt\")\n\nif cfg.get(\"use_fp16\", False) and device == \"cuda\":\n    model = model.half()\n\nmodel.to(device)\nmodel.eval()\n\n# -------------------------\n# Generation Function\n# -------------------------\n@torch.no_grad()\ndef generate(prompt: str):\n    inputs = tokenizer(\n        prompt,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=cfg[\"max_input_length\"],\n    ).to(device)\n\n    generation_kwargs = {\n        \"max_new_tokens\": cfg[\"max_new_tokens\"],\n        \"do_sample\": cfg[\"do_sample\"],\n        \"temperature\": cfg[\"temperature\"],\n        \"top_p\": cfg[\"top_p\"],\n        \"pad_token_id\": tokenizer.eos_token_id,\n        \"repetition_penalty\": cfg[\"repetition_penalty\"],\n    }\n\n    # Optional top-k\n    if cfg.get(\"top_k\") is not None:\n        generation_kwargs[\"top_k\"] = cfg[\"top_k\"]\n\n    outputs = model.generate(**inputs, **generation_kwargs)\n\n    return tokenizer.decode(\n        outputs[0],\n        skip_special_tokens=cfg[\"skip_special_tokens\"]\n    )\n\n# -------------------------\n# Example Run\n# -------------------------\nif __name__ == \"__main__\":\n    prompt = \"Once upon a time in a futuristic city,\"\n    text = generate(prompt)\n    print(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T10:08:18.562797Z","iopub.execute_input":"2026-01-01T10:08:18.563115Z","iopub.status.idle":"2026-01-01T10:08:18.568936Z","shell.execute_reply.started":"2026-01-01T10:08:18.563083Z","shell.execute_reply":"2026-01-01T10:08:18.568408Z"}},"outputs":[{"name":"stdout","text":"Writing inference/inference_amp_ckpt.py\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"!python inference/inference_amp_ckpt.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T10:08:29.704966Z","iopub.execute_input":"2026-01-01T10:08:29.705643Z","iopub.status.idle":"2026-01-01T10:08:43.055229Z","shell.execute_reply.started":"2026-01-01T10:08:29.705617Z","shell.execute_reply":"2026-01-01T10:08:43.054358Z"}},"outputs":[{"name":"stdout","text":"2026-01-01 10:08:35.502188: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767262115.518883     396 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767262115.525291     396 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767262115.541475     396 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767262115.541503     396 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767262115.541507     396 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767262115.541510     396 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nOnce upon a time in a futuristic city, the moon is captured by a man named K 'onk , who uses it to build a spaceship . A crew of three members are stranded in the moon 's atmosphere and must escape from the planet 's orbit . \n\nA series of events lead to the death of the crew , including the discovery of a planetoid , a mysterious alien ship , and the arrival of the mysterious alien ambassador who reveals himself to be a member of the alien alien race . \n\nA female alien named \" K 'onk \" was found in the moon 's atmosphere , and was captured by the aliens and captured . \n\nA female alien named \" K 'onk \" was discovered in the moon 's atmosphere , and was captured by\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"%%writefile train/train_lm_lora.py\n\nimport time\nimport math\nimport yaml\nimport torch\nimport wandb\nfrom torch.utils.data import DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    DataCollatorForLanguageModeling,\n    get_scheduler\n)\n\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n# PEFT LoRA\nfrom peft import LoraConfig, get_peft_model, TaskType\n\n# -------------------------\n# Load config\n# -------------------------\nwith open(\"configs/train_base.yaml\") as f:\n    cfg = yaml.safe_load(f)\n\ntorch.manual_seed(cfg[\"seed\"])\ndevice = torch.device(\"cuda\")\n\n# -------------------------\n# W&B init\n# -------------------------\nwandb.init(project=\"llm-from-scratch\", config=cfg, name=\"LoRA-Training\")\n\n# -------------------------\n# Tokenizer & Model\n# -------------------------\ntokenizer = AutoTokenizer.from_pretrained(cfg[\"model_name\"])\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(cfg[\"model_name\"])\nmodel.gradient_checkpointing_enable()  # gradient checkpointing\nmodel.to(device)\n\nfor name, module in model.named_modules():\n    if \"attn\" in name:\n        print(name)\n\n# -------------------------\n# Freeze base model & Add LoRA\n# -------------------------\nfor param in model.parameters():\n    param.requires_grad = False\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"c_attn\", \"c_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()  # check trainable params\n\n# -------------------------\n# Dataset\n# -------------------------\ndataset = load_dataset(\n    cfg[\"dataset_name\"],\n    cfg[\"dataset_config\"]\n)\n\ndef tokenize_fn(examples):\n    tokens = tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        max_length=cfg[\"max_length\"],\n        padding=False,\n    )\n    input_ids = [ids for ids in tokens[\"input_ids\"] if len(ids) > 0]\n    return {\"input_ids\": input_ids}\n\ntokenized = dataset.map(\n    tokenize_fn,\n    batched=True,\n    remove_columns=[\"text\"]\n)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\ntrain_loader = DataLoader(\n    tokenized[\"train\"],\n    batch_size=cfg[\"batch_size\"],\n    shuffle=True,\n    collate_fn=data_collator\n)\n\n# -------------------------\n# Optimizer & Scheduler\n# -------------------------\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=float(cfg[\"learning_rate\"]),\n    weight_decay=float(cfg[\"weight_decay\"])\n)\n\nnum_training_steps = cfg[\"num_epochs\"] * len(train_loader)\nlr_scheduler = get_scheduler(\n    name=\"cosine\",\n    optimizer=optimizer,\n    num_warmup_steps=int(0.1 * num_training_steps),\n    num_training_steps=num_training_steps\n)\n\nscaler = GradScaler()  # AMP\n\n# -------------------------\n# Training Loop + W&B logging\n# -------------------------\ndef sanity_check(batch):\n    assert batch[\"input_ids\"].dim() == 2\n    assert batch[\"input_ids\"].size(1) > 0\n    \nmodel.train()\nglobal_step = 0\nstep_times = []\n\nfor epoch in range(cfg[\"num_epochs\"]):\n    for batch in tqdm(train_loader):\n        sanity_check(batch)\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        start = time.time()\n        with autocast():\n            outputs = model(**batch)\n            loss = outputs.loss\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        optimizer.zero_grad()\n        lr_scheduler.step()\n        end = time.time()\n\n        step_time = end - start\n        step_times.append(step_time)\n\n        # Log GPU memory\n        max_mem = torch.cuda.max_memory_allocated() / 1024**2  # MB\n        lr = lr_scheduler.get_last_lr()[0]\n\n        # W&B logging\n        wandb.log({\n            \"train/loss\": loss.item(),\n            \"train/perplexity\": math.exp(loss.item()),\n            \"train/lr\": lr,\n            \"train/max_memory_MB\": max_mem,\n            \"train/step_time_s\": step_time,\n            \"train/global_step\": global_step\n        })\n\n        global_step += 1\n\n# -------------------------\n# Save LoRA adapters only\n# -------------------------\nmodel.save_pretrained(cfg[\"output_dir5\"])\ntokenizer.save_pretrained(cfg[\"output_dir5\"])\n\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T10:14:34.826003Z","iopub.execute_input":"2026-01-01T10:14:34.826347Z","iopub.status.idle":"2026-01-01T10:14:34.833037Z","shell.execute_reply.started":"2026-01-01T10:14:34.826315Z","shell.execute_reply":"2026-01-01T10:14:34.832398Z"}},"outputs":[{"name":"stdout","text":"Overwriting train/train_lm_lora.py\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"!python train/train_lm_lora.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T10:14:35.819054Z","iopub.execute_input":"2026-01-01T10:14:35.819712Z","iopub.status.idle":"2026-01-01T10:31:14.798309Z","shell.execute_reply.started":"2026-01-01T10:14:35.819687Z","shell.execute_reply":"2026-01-01T10:31:14.797604Z"}},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n2026-01-01 10:14:41.947568: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767262481.969290     545 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767262481.975624     545 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767262481.992215     545 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767262481.992244     545 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767262481.992247     545 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767262481.992251     545 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrahulkrish28\u001b[0m (\u001b[33mrahulkrish28-california-state-university-fullerton\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20260101_101446-ua3kv7fk\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mLoRA-Training\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/rahulkrish28-california-state-university-fullerton/llm-from-scratch\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/rahulkrish28-california-state-university-fullerton/llm-from-scratch/runs/ua3kv7fk\u001b[0m\ntransformer.h.0.attn\ntransformer.h.0.attn.c_attn\ntransformer.h.0.attn.c_proj\ntransformer.h.0.attn.attn_dropout\ntransformer.h.0.attn.resid_dropout\ntransformer.h.1.attn\ntransformer.h.1.attn.c_attn\ntransformer.h.1.attn.c_proj\ntransformer.h.1.attn.attn_dropout\ntransformer.h.1.attn.resid_dropout\ntransformer.h.2.attn\ntransformer.h.2.attn.c_attn\ntransformer.h.2.attn.c_proj\ntransformer.h.2.attn.attn_dropout\ntransformer.h.2.attn.resid_dropout\ntransformer.h.3.attn\ntransformer.h.3.attn.c_attn\ntransformer.h.3.attn.c_proj\ntransformer.h.3.attn.attn_dropout\ntransformer.h.3.attn.resid_dropout\ntransformer.h.4.attn\ntransformer.h.4.attn.c_attn\ntransformer.h.4.attn.c_proj\ntransformer.h.4.attn.attn_dropout\ntransformer.h.4.attn.resid_dropout\ntransformer.h.5.attn\ntransformer.h.5.attn.c_attn\ntransformer.h.5.attn.c_proj\ntransformer.h.5.attn.attn_dropout\ntransformer.h.5.attn.resid_dropout\ntransformer.h.6.attn\ntransformer.h.6.attn.c_attn\ntransformer.h.6.attn.c_proj\ntransformer.h.6.attn.attn_dropout\ntransformer.h.6.attn.resid_dropout\ntransformer.h.7.attn\ntransformer.h.7.attn.c_attn\ntransformer.h.7.attn.c_proj\ntransformer.h.7.attn.attn_dropout\ntransformer.h.7.attn.resid_dropout\ntransformer.h.8.attn\ntransformer.h.8.attn.c_attn\ntransformer.h.8.attn.c_proj\ntransformer.h.8.attn.attn_dropout\ntransformer.h.8.attn.resid_dropout\ntransformer.h.9.attn\ntransformer.h.9.attn.c_attn\ntransformer.h.9.attn.c_proj\ntransformer.h.9.attn.attn_dropout\ntransformer.h.9.attn.resid_dropout\ntransformer.h.10.attn\ntransformer.h.10.attn.c_attn\ntransformer.h.10.attn.c_proj\ntransformer.h.10.attn.attn_dropout\ntransformer.h.10.attn.resid_dropout\ntransformer.h.11.attn\ntransformer.h.11.attn.c_attn\ntransformer.h.11.attn.c_proj\ntransformer.h.11.attn.attn_dropout\ntransformer.h.11.attn.resid_dropout\n/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n  warnings.warn(\ntrainable params: 811,008 || all params: 125,250,816 || trainable%: 0.6475\nMap: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36718/36718 [00:03<00:00, 10894.23 examples/s]\n/kaggle/working/train/train_lm_lora.py:122: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()  # AMP\n  0%|                                                 | 0/11884 [00:00<?, ?it/s]/kaggle/working/train/train_lm_lora.py:141: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11884/11884 [16:19<00:00, 12.14it/s]\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m updating run metadata (0.0s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m updating run metadata (0.0s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m updating run metadata (0.0s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m updating run metadata (0.0s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m:   train/global_step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà\n\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss ‚ñÜ‚ñÑ‚ñà‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ\n\u001b[34m\u001b[1mwandb\u001b[0m:            train/lr ‚ñÅ‚ñÑ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: train/max_memory_MB ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n\u001b[34m\u001b[1mwandb\u001b[0m:    train/perplexity ‚ñÇ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ\n\u001b[34m\u001b[1mwandb\u001b[0m:   train/step_time_s ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñá‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñá‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÅ‚ñÑ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñà\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m:   train/global_step 11883\n\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss 4.35742\n\u001b[34m\u001b[1mwandb\u001b[0m:            train/lr 0\n\u001b[34m\u001b[1mwandb\u001b[0m: train/max_memory_MB 1318.91553\n\u001b[34m\u001b[1mwandb\u001b[0m:    train/perplexity 78.05545\n\u001b[34m\u001b[1mwandb\u001b[0m:   train/step_time_s 0.07503\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mLoRA-Training\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rahulkrish28-california-state-university-fullerton/llm-from-scratch/runs/ua3kv7fk\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/rahulkrish28-california-state-university-fullerton/llm-from-scratch\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20260101_101446-ua3kv7fk/logs\u001b[0m\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"%%writefile inference/inference_lora.py\n\nimport yaml\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# -------------------------\n# Load Inference Config\n# -------------------------\nwith open(\"configs/inference.yaml\") as f:\n    cfg = yaml.safe_load(f)\n\n# -------------------------\n# Device\n# -------------------------\nif cfg[\"device\"] == \"auto\":\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nelse:\n    device = cfg[\"device\"]\n\n# -------------------------\n# Tokenizer & Model\n# -------------------------\ntokenizer = AutoTokenizer.from_pretrained(\"outputs/train_lm_lora\")\n\n# Pad token handling\nif cfg[\"pad_token\"] == \"eos\":\n    tokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(\"outputs/train_lm_lora\")\n\nif cfg.get(\"use_fp16\", False) and device == \"cuda\":\n    model = model.half()\n\nmodel.to(device)\nmodel.eval()\n\n# -------------------------\n# Generation Function\n# -------------------------\n@torch.no_grad()\ndef generate(prompt: str):\n    inputs = tokenizer(\n        prompt,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=cfg[\"max_input_length\"],\n    ).to(device)\n\n    generation_kwargs = {\n        \"max_new_tokens\": cfg[\"max_new_tokens\"],\n        \"do_sample\": cfg[\"do_sample\"],\n        \"temperature\": cfg[\"temperature\"],\n        \"top_p\": cfg[\"top_p\"],\n        \"pad_token_id\": tokenizer.eos_token_id,\n        \"repetition_penalty\": cfg[\"repetition_penalty\"],\n    }\n\n    # Optional top-k\n    if cfg.get(\"top_k\") is not None:\n        generation_kwargs[\"top_k\"] = cfg[\"top_k\"]\n\n    outputs = model.generate(**inputs, **generation_kwargs)\n\n    return tokenizer.decode(\n        outputs[0],\n        skip_special_tokens=cfg[\"skip_special_tokens\"]\n    )\n\n# -------------------------\n# Example Run\n# -------------------------\nif __name__ == \"__main__\":\n    prompt = \"Once upon a time in a futuristic city,\"\n    text = generate(prompt)\n    print(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T10:32:01.421967Z","iopub.execute_input":"2026-01-01T10:32:01.422318Z","iopub.status.idle":"2026-01-01T10:32:01.428320Z","shell.execute_reply.started":"2026-01-01T10:32:01.422284Z","shell.execute_reply":"2026-01-01T10:32:01.427706Z"}},"outputs":[{"name":"stdout","text":"Writing inference/inference_lora.py\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"!python inference/inference_lora.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T10:32:04.444576Z","iopub.execute_input":"2026-01-01T10:32:04.445192Z","iopub.status.idle":"2026-01-01T10:32:18.946113Z","shell.execute_reply.started":"2026-01-01T10:32:04.445143Z","shell.execute_reply":"2026-01-01T10:32:18.945466Z"}},"outputs":[{"name":"stdout","text":"2026-01-01 10:32:10.331035: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767263530.347789     629 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767263530.353894     629 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767263530.370485     629 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767263530.370512     629 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767263530.370516     629 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767263530.370519     629 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nOnce upon a time in a futuristic city, the people of Earth would seek to survive by adapting to the world around them and adapting to the planet they lived in. The city would provide a common space for them to live in and a place for them to use their creativity and experience to create new forms of life .\n\nThe inhabitants of the city were a mixture of science fiction and fantasy genres . Many of the inhabitants were humanoid , living humanoid creatures . \n\nThey were most common amongst the humans , and the inhabitants were drawn to them as a means to overcome their limitations and their inability to function in the world around them . \n\nThey were also known to have a sense of humor , and were often known to be quite funny . \n\nThey were known to be a\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"%%writefile data/prompts.json\n\n[\n  \"Explain transformers in simple terms.\",\n  \"What is gradient descent?\",\n  \"Why is attention better than RNNs?\",\n  \"Explain LoRA fine-tuning.\",\n  \"What is RLHF?\"\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T10:32:33.300965Z","iopub.execute_input":"2026-01-01T10:32:33.301507Z","iopub.status.idle":"2026-01-01T10:32:33.307342Z","shell.execute_reply.started":"2026-01-01T10:32:33.301455Z","shell.execute_reply":"2026-01-01T10:32:33.306531Z"}},"outputs":[{"name":"stdout","text":"Writing data/prompts.json\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"%%writefile data/generate_preferences.py\n\nimport json\nimport torch\nimport random\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# -------------------------\n# Config\n# -------------------------\nMODEL_NAME = \"gpt2\"\nNUM_SAMPLES = 3        # responses per prompt\nMAX_NEW_TOKENS = 100\nTEMPERATURES = [0.7, 1.0, 1.3]\nOUTPUT_FILE = \"data/preferences.json\"\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# -------------------------\n# Load model\n# -------------------------\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\nmodel.to(device)\nmodel.eval()\n\n# -------------------------\n# Load prompts\n# -------------------------\nwith open(\"data/prompts.json\") as f:\n    prompts = json.load(f)\n\n# -------------------------\n# Generation helper\n# -------------------------\n@torch.no_grad()\ndef generate(prompt, temperature):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    output = model.generate(\n        **inputs,\n        max_new_tokens=MAX_NEW_TOKENS,\n        do_sample=True,\n        temperature=temperature,\n        top_p=0.95\n    )\n    text = tokenizer.decode(output[0], skip_special_tokens=True)\n    return text[len(prompt):].strip()\n\n# -------------------------\n# Simple heuristic scoring\n# -------------------------\ndef heuristic_score(text):\n    \"\"\"\n    Simple automatic preference:\n    - longer is better (up to a point)\n    - penalize very short / empty\n    \"\"\"\n    length = len(text.split())\n    return min(length, 200)\n\n# -------------------------\n# Generate preferences\n# -------------------------\npreference_data = []\n\nfor prompt in tqdm(prompts):\n    candidates = []\n\n    for t in TEMPERATURES[:NUM_SAMPLES]:\n        out = generate(prompt, t)\n        score = heuristic_score(out)\n        candidates.append((out, score))\n\n    candidates.sort(key=lambda x: x[1], reverse=True)\n\n    chosen = candidates[0][0]\n    rejected = candidates[-1][0]\n\n    preference_data.append({\n        \"prompt\": prompt,\n        \"chosen\": chosen,\n        \"rejected\": rejected\n    })\n\n# -------------------------\n# Save\n# -------------------------\nwith open(OUTPUT_FILE, \"w\") as f:\n    json.dump(preference_data, f, indent=2)\n\nprint(f\"Saved {len(preference_data)} preference pairs to {OUTPUT_FILE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T10:34:21.533575Z","iopub.execute_input":"2026-01-01T10:34:21.534114Z","iopub.status.idle":"2026-01-01T10:34:21.539339Z","shell.execute_reply.started":"2026-01-01T10:34:21.534089Z","shell.execute_reply":"2026-01-01T10:34:21.538737Z"}},"outputs":[{"name":"stdout","text":"Writing data/generate_preferences.py\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"!python data/generate_preferences.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T10:34:32.325815Z","iopub.execute_input":"2026-01-01T10:34:32.326485Z","iopub.status.idle":"2026-01-01T10:34:58.161273Z","shell.execute_reply.started":"2026-01-01T10:34:32.326461Z","shell.execute_reply":"2026-01-01T10:34:58.160571Z"}},"outputs":[{"name":"stdout","text":"2026-01-01 10:34:38.418987: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767263678.436588     661 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767263678.441614     661 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767263678.458245     661 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767263678.458275     661 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767263678.458279     661 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767263678.458282     661 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n  0%|                                                     | 0/5 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                    | 1/5 [00:03<00:12,  3.23s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                           | 2/5 [00:05<00:08,  2.89s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 3/5 [00:08<00:05,  2.79s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 4/5 [00:11<00:02,  2.74s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:13<00:00,  2.78s/it]\nSaved 5 preference pairs to data/preferences.json\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"%%writefile data/generate_preferences_multi.py\n\nimport json\nimport torch\nimport random\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# -------------------------\n# Config\n# -------------------------\nMODEL_NAME = \"gpt2\"\nNUM_CANDIDATES = 6        # total generations per prompt\nNUM_REJECTIONS = 3        # how many rejected to keep\nMAX_NEW_TOKENS = 120\nTEMPERATURES = [0.7, 0.9, 1.1, 1.3]\nTOP_P = 0.95\n\nPROMPTS_FILE = \"data/prompts.json\"\nOUTPUT_FILE = \"data/preferences_multi.json\"\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# -------------------------\n# Load model\n# -------------------------\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\nmodel.to(device)\nmodel.eval()\n\n# -------------------------\n# Load prompts\n# -------------------------\nwith open(PROMPTS_FILE) as f:\n    prompts = json.load(f)\n\n# -------------------------\n# Generation helper\n# -------------------------\n@torch.no_grad()\ndef generate(prompt, temperature):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=MAX_NEW_TOKENS,\n        do_sample=True,\n        temperature=temperature,\n        top_p=TOP_P\n    )\n    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return text[len(prompt):].strip()\n\n# -------------------------\n# Heuristic scoring\n# -------------------------\ndef score_response(text):\n    \"\"\"\n    Simple but effective:\n    - Prefer medium-length answers\n    - Penalize very short / very long\n    \"\"\"\n    length = len(text.split())\n    if length < 10:\n        return -10\n    if length > 200:\n        return 200 - length\n    return length\n\n# -------------------------\n# Generate multi-rejection preferences\n# -------------------------\npreference_data = []\n\nfor prompt in tqdm(prompts):\n    candidates = []\n\n    for i in range(NUM_CANDIDATES):\n        temp = random.choice(TEMPERATURES)\n        response = generate(prompt, temp)\n        score = score_response(response)\n        candidates.append((response, score))\n\n    # Sort best ‚Üí worst\n    candidates.sort(key=lambda x: x[1], reverse=True)\n\n    chosen = candidates[0][0]\n    rejected = [c[0] for c in candidates[-NUM_REJECTIONS:]]\n\n    preference_data.append({\n        \"prompt\": prompt,\n        \"chosen\": chosen,\n        \"rejected\": rejected\n    })\n\n# -------------------------\n# Save\n# -------------------------\nwith open(OUTPUT_FILE, \"w\") as f:\n    json.dump(preference_data, f, indent=2)\n\nprint(f\"Saved {len(preference_data)} multi-rejection examples ‚Üí {OUTPUT_FILE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T10:35:07.379632Z","iopub.execute_input":"2026-01-01T10:35:07.380184Z","iopub.status.idle":"2026-01-01T10:35:07.386548Z","shell.execute_reply.started":"2026-01-01T10:35:07.380135Z","shell.execute_reply":"2026-01-01T10:35:07.385827Z"}},"outputs":[{"name":"stdout","text":"Writing data/generate_preferences_multi.py\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"!python data/generate_preferences_multi.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T10:35:18.508861Z","iopub.execute_input":"2026-01-01T10:35:18.509548Z","iopub.status.idle":"2026-01-01T10:36:00.868966Z","shell.execute_reply.started":"2026-01-01T10:35:18.509521Z","shell.execute_reply":"2026-01-01T10:36:00.868228Z"}},"outputs":[{"name":"stdout","text":"2026-01-01 10:35:24.636037: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767263724.654334     685 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767263724.659315     685 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767263724.675689     685 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767263724.675717     685 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767263724.675721     685 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767263724.675724     685 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n  0%|                                                     | 0/5 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                    | 1/5 [00:06<00:27,  6.83s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                           | 2/5 [00:13<00:19,  6.55s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 3/5 [00:19<00:12,  6.47s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 4/5 [00:24<00:05,  5.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:30<00:00,  6.08s/it]\nSaved 5 multi-rejection examples ‚Üí data/preferences_multi.json\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"%%writefile rlhf/__init__.py\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T10:41:22.655002Z","iopub.execute_input":"2026-01-01T10:41:22.655307Z","iopub.status.idle":"2026-01-01T10:41:22.659845Z","shell.execute_reply.started":"2026-01-01T10:41:22.655284Z","shell.execute_reply":"2026-01-01T10:41:22.659154Z"}},"outputs":[{"name":"stdout","text":"Writing rlhf/__init__.py\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"%%writefile rlhf/dataset.py\n\nfrom torch.utils.data import Dataset\n\"\"\"Dataset for preference-based learning from human feedback.\nPreference Dataset Format:\n{\n  \"prompt\": \"Explain transformers\",\n  \"chosen\": \"Transformers use attention...\",\n  \"rejected\": \"Transformers are like RNNs...\"\n}\n\"\"\"\nclass PreferenceDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def encode(self, text):\n        return self.tokenizer(\n            text,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n\n        chosen = self.encode(item[\"prompt\"] + item[\"chosen\"])\n        rejected = self.encode(item[\"prompt\"] + item[\"rejected\"])\n\n        return {\n            \"chosen_input_ids\": chosen[\"input_ids\"].squeeze(0),\n            \"chosen_attention_mask\": chosen[\"attention_mask\"].squeeze(0),\n            \"rejected_input_ids\": rejected[\"input_ids\"].squeeze(0),\n            \"rejected_attention_mask\": rejected[\"attention_mask\"].squeeze(0),\n        }\n\nclass MultiPreferenceDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def encode(self, text):\n        return self.tokenizer(\n            text,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n\n        chosen = self.encode(item[\"prompt\"] + item[\"chosen\"])\n\n        rejected = [\n            self.encode(item[\"prompt\"] + r)\n            for r in item[\"rejected\"]\n        ]\n\n        return {\n            \"chosen\": {\n                \"input_ids\": chosen[\"input_ids\"].squeeze(0),\n                \"attention_mask\": chosen[\"attention_mask\"].squeeze(0),\n            },\n            \"rejected\": [\n                {\n                    \"input_ids\": r[\"input_ids\"].squeeze(0),\n                    \"attention_mask\": r[\"attention_mask\"].squeeze(0),\n                }\n                for r in rejected\n            ]\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T10:41:25.822784Z","iopub.execute_input":"2026-01-01T10:41:25.823363Z","iopub.status.idle":"2026-01-01T10:41:25.828614Z","shell.execute_reply.started":"2026-01-01T10:41:25.823333Z","shell.execute_reply":"2026-01-01T10:41:25.828003Z"}},"outputs":[{"name":"stdout","text":"Overwriting rlhf/dataset.py\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"%%writefile rlhf/reward_model.py\n\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoModel\n\nclass RewardModel(nn.Module):\n    def __init__(self, base_model_name):\n        super().__init__()\n        self.backbone = AutoModel.from_pretrained(base_model_name)\n        hidden_size = self.backbone.config.hidden_size\n        self.reward_head = nn.Linear(hidden_size, 1)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.backbone(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        pooled = outputs.last_hidden_state[:, -1]\n        reward = self.reward_head(pooled)\n        return reward\n\n    def save_pretrained(self, save_directory):\n        self.backbone.save_pretrained(save_directory)\n        torch.save(\n            self.reward_head.state_dict(),\n            f\"{save_directory}/reward_head.pt\"\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T10:45:16.371828Z","iopub.execute_input":"2026-01-01T10:45:16.372377Z","iopub.status.idle":"2026-01-01T10:45:16.377715Z","shell.execute_reply.started":"2026-01-01T10:45:16.372342Z","shell.execute_reply":"2026-01-01T10:45:16.377057Z"}},"outputs":[{"name":"stdout","text":"Overwriting rlhf/reward_model.py\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"%%writefile rlhf/train_reward_model.py\n\nimport yaml\nimport torch\nimport wandb\nimport torch.nn.functional as F\n\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\nfrom rlhf.reward_model import RewardModel\nfrom rlhf.dataset import PreferenceDataset\n\n# -------------------------\n# Config\n# -------------------------\nwith open(\"configs/train_base.yaml\") as f:\n    cfg = yaml.safe_load(f)\n\ndevice = torch.device(\"cuda\")\nwandb.init(project=\"llm-from-scratch\", name=\"reward-model\")\n\n# -------------------------\n# Data\n# -------------------------\ntokenizer = AutoTokenizer.from_pretrained(\"outputs/train_lm\")\ndataset = load_dataset(\"json\", data_files=\"data/preferences.json\")[\"train\"]\n\npref_dataset = PreferenceDataset(\n    dataset,\n    tokenizer,\n    cfg[\"max_length\"]\n)\n\nloader = DataLoader(pref_dataset, batch_size=cfg[\"batch_size\"], shuffle=True)\n\n# -------------------------\n# Model\n# -------------------------\nmodel = RewardModel(\"outputs/train_lm\").to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\n# -------------------------\n# Pairwise Ranking Loss\n# -------------------------\ndef reward_loss(r_chosen, r_rejected):\n    return -F.logsigmoid(r_chosen - r_rejected).mean()\n\n# -------------------------\n# Training Loop\n# -------------------------\nmodel.train()\nfor epoch in range(3):\n    for batch in tqdm(loader):\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        r_chosen = model(\n            batch[\"chosen_input_ids\"],\n            batch[\"chosen_attention_mask\"]\n        )\n        r_rejected = model(\n            batch[\"rejected_input_ids\"],\n            batch[\"rejected_attention_mask\"]\n        )\n\n        loss = reward_loss(r_chosen, r_rejected)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        wandb.log({\"reward_loss\": loss.item()})\n\nmodel.save_pretrained(\"models/reward_model\")\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T10:45:21.271941Z","iopub.execute_input":"2026-01-01T10:45:21.272612Z","iopub.status.idle":"2026-01-01T10:45:21.277488Z","shell.execute_reply.started":"2026-01-01T10:45:21.272588Z","shell.execute_reply":"2026-01-01T10:45:21.276918Z"}},"outputs":[{"name":"stdout","text":"Overwriting rlhf/train_reward_model.py\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"!python -m rlhf.train_reward_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T10:45:24.843643Z","iopub.execute_input":"2026-01-01T10:45:24.844330Z","iopub.status.idle":"2026-01-01T10:45:44.588961Z","shell.execute_reply.started":"2026-01-01T10:45:24.844304Z","shell.execute_reply":"2026-01-01T10:45:44.588070Z"}},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrahulkrish28\u001b[0m (\u001b[33mrahulkrish28-california-state-university-fullerton\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20260101_104531-1q66xlq1\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mreward-model\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/rahulkrish28-california-state-university-fullerton/llm-from-scratch\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/rahulkrish28-california-state-university-fullerton/llm-from-scratch/runs/1q66xlq1\u001b[0m\n2026-01-01 10:45:33.502534: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767264333.520215     796 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767264333.525199     796 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767264333.538378     796 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767264333.538403     796 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767264333.538406     796 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767264333.538410     796 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  2.02it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  2.69it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  2.72it/s]\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m updating run metadata (0.0s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m updating run metadata (0.0s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m updating run metadata (0.0s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m updating run metadata (0.0s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m updating run metadata (0.0s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ø\u001b[0m updating run metadata (0.0s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m: reward_loss ‚ñÑ‚ñà‚ñÇ‚ñÇ‚ñÜ‚ñÉ‚ñÜ‚ñÇ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m: reward_loss 0.02523\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mreward-model\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rahulkrish28-california-state-university-fullerton/llm-from-scratch/runs/1q66xlq1\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/rahulkrish28-california-state-university-fullerton/llm-from-scratch\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20260101_104531-1q66xlq1/logs\u001b[0m\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"%%writefile rlhf/utils.py\n\nimport torch\n\ndef sequence_logprob(model, input_ids, attention_mask):\n    outputs = model(\n        input_ids=input_ids,\n        attention_mask=attention_mask\n    )\n    logits = outputs.logits[:, :-1]\n    labels = input_ids[:, 1:]\n\n    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n    token_logprobs = log_probs.gather(\n        2, labels.unsqueeze(-1)\n    ).squeeze(-1)\n\n    mask = attention_mask[:, 1:]\n    return (token_logprobs * mask).sum(dim=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T10:45:51.180969Z","iopub.execute_input":"2026-01-01T10:45:51.181760Z","iopub.status.idle":"2026-01-01T10:45:51.186476Z","shell.execute_reply.started":"2026-01-01T10:45:51.181726Z","shell.execute_reply":"2026-01-01T10:45:51.185751Z"}},"outputs":[{"name":"stdout","text":"Writing rlhf/utils.py\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"%%writefile rlhf/ppo_train.py\n\nimport yaml\nimport torch\nimport wandb\nimport torch.nn.functional as F\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom rlhf.reward_model import RewardModel\nfrom rlhf.dataset import PreferenceDataset\nfrom rlhf.utils import sequence_logprob\n\n# -------------------------\n# Config\n# -------------------------\nwith open(\"configs/train_base.yaml\") as f:\n    cfg = yaml.safe_load(f)\n\ndevice = \"cuda\"\nwandb.init(project=\"llm-from-scratch\", name=\"PPO-LoRA\")\n\n# -------------------------\n# Models\n# -------------------------\ntokenizer = AutoTokenizer.from_pretrained(\"outputs/train_lm\")\ntokenizer.pad_token = tokenizer.eos_token\n\npolicy = AutoModelForCausalLM.from_pretrained(\"outputs/train_lm\")\n\nref = AutoModelForCausalLM.from_pretrained(\"outputs/train_lm\").to(device)\nref.eval()\nfor p in ref.parameters():\n    p.requires_grad = False\n\nreward_model = RewardModel(\"outputs/train_lm\").to(device)\nreward_model.reward_head.load_state_dict(\n    torch.load(\"models/reward_model/reward_head.pt\", map_location=device)\n)\nreward_model.eval()\n\nlora_cfg = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"c_attn\", \"c_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM\n)\n\npolicy = get_peft_model(policy, lora_cfg).to(device)\noptimizer = torch.optim.AdamW(policy.parameters(), lr=1e-5)\n\n# -------------------------\n# Data\n# -------------------------\ndataset = load_dataset(\"json\", data_files=\"data/preferences.json\")[\"train\"]\nloader = DataLoader(\n    PreferenceDataset(dataset, tokenizer, cfg[\"max_length\"]),\n    batch_size=cfg[\"batch_size\"],\n    shuffle=True\n)\n\n# -------------------------\n# PPO Training.        L=min(rt ‚ÄãAt‚Äã,clip(rt‚Äã,1‚àíœµ,1+œµ) At‚Äã)‚àí Œ≤KL(œÄ‚à£‚à£œÄref‚Äã)\n# -------------------------\nclip_eps = 0.2\nkl_beta = 0.1\n\nfor epoch in range(3):\n    for batch in tqdm(loader):\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        logp_new = sequence_logprob(\n            policy,\n            batch[\"chosen_input_ids\"],\n            batch[\"chosen_attention_mask\"]\n        )\n\n        with torch.no_grad():\n            logp_old = sequence_logprob(\n                ref,\n                batch[\"chosen_input_ids\"],\n                batch[\"chosen_attention_mask\"]\n            )\n            reward = reward_model(\n                batch[\"chosen_input_ids\"],\n                batch[\"chosen_attention_mask\"]\n            ).squeeze()\n\n        ratio = torch.exp(logp_new - logp_old)\n        advantage = reward - reward.mean()\n        \n        unclipped = ratio * advantage\n        clipped = torch.clamp(ratio, 1 - clip_eps, 1 + clip_eps) * advantage\n        \n        policy_loss = -torch.min(unclipped, clipped).mean()\n        kl = (logp_new - logp_old).mean()\n        \n        loss = policy_loss + kl_beta * kl\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        wandb.log({\n            \"ppo/loss\": loss.item(),\n            \"ppo/reward\": reward.mean().item(),\n            \"ppo/kl\": kl.item()\n        })\n\npolicy.save_pretrained(\"models/ppo_lora\")\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T11:09:06.183273Z","iopub.execute_input":"2026-01-01T11:09:06.184028Z","iopub.status.idle":"2026-01-01T11:09:06.190300Z","shell.execute_reply.started":"2026-01-01T11:09:06.183993Z","shell.execute_reply":"2026-01-01T11:09:06.189671Z"}},"outputs":[{"name":"stdout","text":"Overwriting rlhf/ppo_train.py\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"!python -m rlhf.ppo_train.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T11:09:06.623354Z","iopub.execute_input":"2026-01-01T11:09:06.624005Z","iopub.status.idle":"2026-01-01T11:09:25.363247Z","shell.execute_reply.started":"2026-01-01T11:09:06.623981Z","shell.execute_reply":"2026-01-01T11:09:25.362539Z"}},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n2026-01-01 11:09:12.989092: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767265753.010385    1120 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767265753.016771    1120 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767265753.033203    1120 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767265753.033231    1120 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767265753.033234    1120 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767265753.033238    1120 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrahulkrish28\u001b[0m (\u001b[33mrahulkrish28-california-state-university-fullerton\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20260101_110917-8twgnjne\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mPPO-LoRA\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/rahulkrish28-california-state-university-fullerton/llm-from-scratch\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/rahulkrish28-california-state-university-fullerton/llm-from-scratch/runs/8twgnjne\u001b[0m\n/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n  warnings.warn(\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  2.09it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  3.21it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  3.20it/s]\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m updating run metadata (5.1s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m updating run metadata (5.1s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m updating run metadata (5.1s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m updating run metadata (5.1s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m updating run metadata (5.1s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m:     ppo/kl ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÑ‚ñÖ‚ñá‚ñá‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:   ppo/loss ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÇ‚ñÑ‚ñà‚ñÅ‚ñÇ\n\u001b[34m\u001b[1mwandb\u001b[0m: ppo/reward ‚ñÉ‚ñÖ‚ñà‚ñÜ‚ñÇ‚ñà‚ñá‚ñÖ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m:     ppo/kl -0.3266\n\u001b[34m\u001b[1mwandb\u001b[0m:   ppo/loss -0.03266\n\u001b[34m\u001b[1mwandb\u001b[0m: ppo/reward -2.62112\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mPPO-LoRA\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rahulkrish28-california-state-university-fullerton/llm-from-scratch/runs/8twgnjne\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/rahulkrish28-california-state-university-fullerton/llm-from-scratch\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20260101_110917-8twgnjne/logs\u001b[0m\n/usr/bin/python3: Error while finding module specification for 'rlhf.ppo_train.py' (ModuleNotFoundError: __path__ attribute not found on 'rlhf.ppo_train' while trying to find 'rlhf.ppo_train.py'). Try using 'rlhf.ppo_train' instead of 'rlhf.ppo_train.py' as the module name.\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"%%writefile inference/ppo_inference.py\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\nfrom rlhf.reward_model import RewardModel\nfrom rlhf.utils import sequence_logprob\n\ndevice = \"cuda\"\n\ntokenizer = AutoTokenizer.from_pretrained(\"outputs/train_lm\")\ntokenizer.pad_token = tokenizer.eos_token\n\nbase = AutoModelForCausalLM.from_pretrained(\"outputs/train_lm\").to(device)\npolicy = PeftModel.from_pretrained(base, \"models/ppo_lora\").to(device)\npolicy.eval()\n\n@torch.no_grad()\ndef generate(model, prompt, max_new_tokens=100):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    output = model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        do_sample=True,\n        temperature=0.7,\n        top_p=0.9\n    )\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\nprompt = \"Explain reinforcement learning from human feedback.\"\n\nprint(\"=== Base Model ===\")\nprint(generate(base, prompt))\n\nprint(\"\\n=== PPO-RLHF Model ===\")\nprint(generate(policy, prompt))\n\nreward_model = RewardModel(\"outputs/train_lm\").to(device)\nreward_model.reward_head.load_state_dict(\n    torch.load(\"models/reward_model/reward_head.pt\", map_location=device)\n)\nreward_model.eval()\n\n@torch.no_grad()\ndef score(text):\n    tokens = tokenizer(\n        text,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=512\n    ).to(device)\n    return reward_model(\n        tokens[\"input_ids\"],\n        tokens[\"attention_mask\"]\n    ).item()\n\nbase_out = generate(base, prompt)\nppo_out = generate(policy, prompt)\n\nprint(\"Reward(Base):\", score(base_out))\nprint(\"Reward(PPO): \", score(ppo_out))\n\n\n@torch.no_grad()\ndef kl_div(prompt):\n    tokens = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    logp_policy = sequence_logprob(\n        policy, tokens[\"input_ids\"], tokens[\"attention_mask\"]\n    )\n    logp_ref = sequence_logprob(\n        base, tokens[\"input_ids\"], tokens[\"attention_mask\"]\n    )\n    return (logp_policy - logp_ref).item()\n\nprint(\"KL divergence:\", kl_div(prompt))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T11:09:29.010223Z","iopub.execute_input":"2026-01-01T11:09:29.010528Z","iopub.status.idle":"2026-01-01T11:09:29.016655Z","shell.execute_reply.started":"2026-01-01T11:09:29.010497Z","shell.execute_reply":"2026-01-01T11:09:29.015928Z"}},"outputs":[{"name":"stdout","text":"Overwriting inference/ppo_inference.py\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"!python -m inference.ppo_inference","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T11:09:29.581260Z","iopub.execute_input":"2026-01-01T11:09:29.581517Z","iopub.status.idle":"2026-01-01T11:09:47.428176Z","shell.execute_reply.started":"2026-01-01T11:09:29.581495Z","shell.execute_reply":"2026-01-01T11:09:47.427515Z"}},"outputs":[{"name":"stdout","text":"2026-01-01 11:09:35.145737: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767265775.168221    1173 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767265775.174556    1173 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767265775.190874    1173 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767265775.190903    1173 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767265775.190908    1173 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767265775.190913    1173 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n=== Base Model ===\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nExplain reinforcement learning from human feedback. \n\nCitation = \n\n@ \n\n@ \n\n@ \n = \n = \n\n = \n\n = \n = \n\n = \n = \n\n \n\n \n = \n\n = \n = \n = \n = \n = \n = \n\n \n \n = \n\n  \n = \n = \n\n \n  \n\n  \n\n\n=== PPO-RLHF Model ===\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nExplain reinforcement learning from human feedback. \n\nPrologue \n\nCitation \n = \n\nPractical applications of reinforcement learning for the construction of machine learning models \n\n \n\nElements of reinforcement learning \n = \n\nPrologue \n = \n = \n = \n = \n = \n = \n = \n = \n = \n = \n = \n = \n = \n =  \n = \n = \nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nReward(Base): -2.913774013519287\nReward(PPO):  -0.27870437502861023\nKL divergence: 0.0\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"%%writefile rlhf/dpo_train.py\n\nimport yaml\nimport torch\nimport wandb\nimport torch.nn.functional as F\n\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom rlhf.dataset import PreferenceDataset\n\n# -------------------------\n# Config\n# -------------------------\nwith open(\"configs/train_base.yaml\") as f:\n    cfg = yaml.safe_load(f)\n\ndevice = torch.device(\"cuda\")\nwandb.init(project=\"llm-from-scratch\", name=\"DPO-LoRA\")\n\n# -------------------------\n# Tokenizer & Model\n# -------------------------\ntokenizer = AutoTokenizer.from_pretrained(\"outputs/train_lm\")\ntokenizer.pad_token = tokenizer.eos_token\n\nbase_model = AutoModelForCausalLM.from_pretrained(\"outputs/train_lm\")\nbase_model.to(device)\nbase_model.eval()  # reference model\n\npolicy_model = AutoModelForCausalLM.from_pretrained(\"outputs/train_lm\")\n\nlora_cfg = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"c_attn\", \"c_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM\n)\n\npolicy_model = get_peft_model(policy_model, lora_cfg).to(device)\n\n# -------------------------\n# Data\n# -------------------------\ndataset = load_dataset(\"json\", data_files=\"data/preferences.json\")[\"train\"]\npref_dataset = PreferenceDataset(dataset, tokenizer, cfg[\"max_length\"])\nloader = DataLoader(pref_dataset, batch_size=cfg[\"batch_size\"], shuffle=True)\n\noptimizer = torch.optim.AdamW(policy_model.parameters(), lr=1e-5)\n\n# -------------------------\n# DPO Loss.      logœÉ(Œ≤[(œÄc‚Äã‚àíœÄr‚Äã)‚àí(œÄref,c‚Äã‚àí œÄref,r‚Äã)])\n# -------------------------\ndef dpo_loss(policy_chosen, policy_rejected,\n             ref_chosen, ref_rejected,\n             beta=0.1):\n    pi_logratios = policy_chosen - policy_rejected\n    ref_logratios = ref_chosen - ref_rejected\n    return -F.logsigmoid(beta * (pi_logratios - ref_logratios)).mean()\n\n# -------------------------\n# Training Loop\n# -------------------------\npolicy_model.train()\nwith torch.no_grad():\n    base_model.eval()\n\nfor epoch in range(3):\n    for batch in tqdm(loader):\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        def logp(model, ids, mask):\n            out = model(input_ids=ids, attention_mask=mask)\n            return out.logits[:, -1, :].log_softmax(-1).mean()\n\n        p_c = logp(policy_model,\n                   batch[\"chosen_input_ids\"],\n                   batch[\"chosen_attention_mask\"])\n        p_r = logp(policy_model,\n                   batch[\"rejected_input_ids\"],\n                   batch[\"rejected_attention_mask\"])\n\n        with torch.no_grad():\n            r_c = logp(base_model,\n                       batch[\"chosen_input_ids\"],\n                       batch[\"chosen_attention_mask\"])\n            r_r = logp(base_model,\n                       batch[\"rejected_input_ids\"],\n                       batch[\"rejected_attention_mask\"])\n\n        loss = dpo_loss(p_c, p_r, r_c, r_r)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        wandb.log({\"dpo_loss\": loss.item()})\n\npolicy_model.save_pretrained(\"models/dpo_lora\")\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T11:11:08.434238Z","iopub.execute_input":"2026-01-01T11:11:08.434814Z","iopub.status.idle":"2026-01-01T11:11:08.441132Z","shell.execute_reply.started":"2026-01-01T11:11:08.434782Z","shell.execute_reply":"2026-01-01T11:11:08.440485Z"}},"outputs":[{"name":"stdout","text":"Writing rlhf/dpo_train.py\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"!python -m rlhf.dpo_train.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T11:11:38.640385Z","iopub.execute_input":"2026-01-01T11:11:38.641080Z","iopub.status.idle":"2026-01-01T11:11:58.879666Z","shell.execute_reply.started":"2026-01-01T11:11:38.641055Z","shell.execute_reply":"2026-01-01T11:11:58.878982Z"}},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n2026-01-01 11:11:45.622735: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767265905.644600    1196 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767265905.651093    1196 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767265905.668494    1196 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767265905.668524    1196 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767265905.668529    1196 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767265905.668535    1196 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrahulkrish28\u001b[0m (\u001b[33mrahulkrish28-california-state-university-fullerton\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20260101_111149-vj7ao914\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mDPO-LoRA\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/rahulkrish28-california-state-university-fullerton/llm-from-scratch\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/rahulkrish28-california-state-university-fullerton/llm-from-scratch/runs/vj7ao914\u001b[0m\n/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n  warnings.warn(\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.62it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  2.01it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  2.00it/s]\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m updating run metadata (0.0s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m updating run metadata (0.0s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m updating run metadata (0.0s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m updating run metadata (0.0s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m: dpo_loss ‚ñÅ‚ñÑ‚ñá‚ñÉ‚ñÅ‚ñÇ‚ñÜ‚ñà‚ñÑ\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m: dpo_loss 0.72439\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mDPO-LoRA\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rahulkrish28-california-state-university-fullerton/llm-from-scratch/runs/vj7ao914\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/rahulkrish28-california-state-university-fullerton/llm-from-scratch\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20260101_111149-vj7ao914/logs\u001b[0m\n/usr/bin/python3: Error while finding module specification for 'rlhf.dpo_train.py' (ModuleNotFoundError: __path__ attribute not found on 'rlhf.dpo_train' while trying to find 'rlhf.dpo_train.py'). Try using 'rlhf.dpo_train' instead of 'rlhf.dpo_train.py' as the module name.\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"%%writefile inference/dpo_inference.py\n\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\nfrom tqdm import tqdm\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# -------------------------\n# Load tokenizer\n# -------------------------\ntokenizer = AutoTokenizer.from_pretrained(\"outputs/train_lm\")\ntokenizer.pad_token = tokenizer.eos_token\n\n# -------------------------\n# Load base (reference) model\n# -------------------------\nbase_model = AutoModelForCausalLM.from_pretrained(\"outputs/train_lm\").to(device)\nbase_model.eval()\n\n# -------------------------\n# Load DPO policy (base + LoRA)\n# -------------------------\npolicy_model = AutoModelForCausalLM.from_pretrained(\"outputs/train_lm\")\npolicy_model = PeftModel.from_pretrained(policy_model, \"models/dpo_lora\")\npolicy_model = policy_model.to(device)\npolicy_model.eval()\n\n# -------------------------\n# Helper: sequence log-prob\n# -------------------------\ndef sequence_logprob(model, input_ids, attention_mask):\n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits[:, :-1]\n        labels = input_ids[:, 1:]\n\n        log_probs = F.log_softmax(logits, dim=-1)\n        token_logp = log_probs.gather(\n            -1, labels.unsqueeze(-1)\n        ).squeeze(-1)\n\n        token_logp = token_logp * attention_mask[:, 1:]\n        return token_logp.sum(dim=1)  # [B]\n\n# -------------------------\n# Generation\n# -------------------------\ndef generate(model, prompt, max_new_tokens=100):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        do_sample=True,\n        temperature=0.7,\n        top_p=0.9\n    )\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# -------------------------\n# Evaluation example\n# -------------------------\nprompt = \"Explain why reinforcement learning is useful for LLM alignment.\"\n\nbase_text = generate(base_model, prompt)\npolicy_text = generate(policy_model, prompt)\n\nprint(\"\\n=== PROMPT ===\")\nprint(prompt)\n\nprint(\"\\n=== BASE MODEL ===\")\nprint(base_text)\n\nprint(\"\\n=== DPO MODEL ===\")\nprint(policy_text)\n\n# -------------------------\n# DPO preference score\n# -------------------------\ninputs_base = tokenizer(base_text, return_tensors=\"pt\", truncation=True).to(device)\ninputs_policy = tokenizer(policy_text, return_tensors=\"pt\", truncation=True).to(device)\n\nlogp_base = sequence_logprob(\n    base_model,\n    inputs_base[\"input_ids\"],\n    inputs_base[\"attention_mask\"]\n)\n\nlogp_policy = sequence_logprob(\n    policy_model,\n    inputs_policy[\"input_ids\"],\n    inputs_policy[\"attention_mask\"]\n)\n\nprint(\"\\n=== DPO Preference Score ===\")\nprint(f\"Policy ‚àí Base logp: {(logp_policy - logp_base).item():.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T11:13:39.893045Z","iopub.execute_input":"2026-01-01T11:13:39.893467Z","iopub.status.idle":"2026-01-01T11:13:39.900042Z","shell.execute_reply.started":"2026-01-01T11:13:39.893435Z","shell.execute_reply":"2026-01-01T11:13:39.899371Z"}},"outputs":[{"name":"stdout","text":"Writing inference/dpo_inference.py\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"!python -m inference.dpo_inference.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T11:13:54.167896Z","iopub.execute_input":"2026-01-01T11:13:54.168369Z","iopub.status.idle":"2026-01-01T11:14:09.097047Z","shell.execute_reply.started":"2026-01-01T11:13:54.168343Z","shell.execute_reply":"2026-01-01T11:14:09.096228Z"}},"outputs":[{"name":"stdout","text":"2026-01-01 11:13:59.901845: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767266039.923128    1250 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767266039.929631    1250 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767266039.946861    1250 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767266039.946891    1250 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767266039.946896    1250 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767266039.946901    1250 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n=== PROMPT ===\nExplain why reinforcement learning is useful for LLM alignment.\n\n=== BASE MODEL ===\nExplain why reinforcement learning is useful for LLM alignment. \n\n. \n\nA final note on reinforcement learning : \n\nI had to learn how to manipulate the maze to a certain degree of success , which is why I was able to solve the maze without much trouble . \n = \n = \n = \n = \n = \n = \n = \n = \n = \n \n \n \n \n \n\n \n \n\n \n \n = \n =\n\n=== DPO MODEL ===\nExplain why reinforcement learning is useful for LLM alignment. \n\n@ \n\n@ \n\n@ \n\n@ \n = \n = \n = \n\n@ \n = \n = \n\n= \n\n = \n = \n = \n = \n\n = \n\n = \n = \n = \n = \n\n = \n = \n\n  \n = \n = \n\n  \n\n \n = \n\n \n\n=== DPO Preference Score ===\nPolicy ‚àí Base logp: 10.8195\n/usr/bin/python3: Error while finding module specification for 'inference.dpo_inference.py' (ModuleNotFoundError: __path__ attribute not found on 'inference.dpo_inference' while trying to find 'inference.dpo_inference.py'). Try using 'inference.dpo_inference' instead of 'inference.dpo_inference.py' as the module name.\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"%%writefile rlhf/grpo_train.py\n\nimport yaml\nimport torch\nimport wandb\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom rlhf.dataset import MultiPreferenceDataset\nfrom rlhf.utils import sequence_logprob\n\n# -------------------------\n# Config\n# -------------------------\nwith open(\"configs/train_base.yaml\") as f:\n    cfg = yaml.safe_load(f)\n\ndevice = \"cuda\"\nwandb.init(project=\"llm-from-scratch\", name=\"GRPO-LoRA-Multi\")\n\n# -------------------------\n# Model\n# -------------------------\ntokenizer = AutoTokenizer.from_pretrained(\"outputs/train_lm\")\ntokenizer.pad_token = tokenizer.eos_token\n\npolicy = AutoModelForCausalLM.from_pretrained(\"outputs/train_lm\")\npolicy = get_peft_model(\n    policy,\n    LoraConfig(\n        r=8,\n        lora_alpha=16,\n        target_modules=[\"c_attn\", \"c_proj\"],\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=TaskType.CAUSAL_LM\n    )\n).to(device)\n\noptimizer = torch.optim.AdamW(policy.parameters(), lr=1e-5)\n\n# -------------------------\n# Data\n# -------------------------\ndataset = load_dataset(\n    \"json\",\n    data_files=\"data/preferences_multi.json\"\n)[\"train\"]\n\nloader = DataLoader(\n    MultiPreferenceDataset(dataset, tokenizer, cfg[\"max_length\"]),\n    batch_size=cfg[\"batch_size\"],\n    shuffle=True\n)\n\n# -------------------------\n# GRPO Training (TRUE GROUP VERSION)\n# -------------------------\npolicy.train()\n\nfor epoch in range(3):\n    for batch in tqdm(loader):\n\n        # ----- chosen -----\n        chosen_ids = batch[\"chosen\"][\"input_ids\"].to(device)\n        chosen_mask = batch[\"chosen\"][\"attention_mask\"].to(device)\n\n        lp_chosen = sequence_logprob(\n            policy,\n            chosen_ids,\n            chosen_mask\n        )\n\n        # ----- rejected group -----\n        rejected_logps = []\n\n        for r in batch[\"rejected\"]:\n            r_ids = r[\"input_ids\"].to(device)\n            r_mask = r[\"attention_mask\"].to(device)\n\n            lp_r = sequence_logprob(\n                policy,\n                r_ids,\n                r_mask\n            )\n            rejected_logps.append(lp_r)\n\n        # Shape: [B, K]\n        rejected_logps = torch.stack(rejected_logps, dim=1)\n\n        # ----- group normalization -----\n        all_scores = torch.cat(\n            [lp_chosen.unsqueeze(1), rejected_logps],\n            dim=1\n        )  # [B, 1 + K]\n\n        advantages = all_scores - all_scores.mean(dim=1, keepdim=True)\n\n        # ----- GRPO loss -----\n        loss = -advantages[:, 0].mean()\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        wandb.log({\n            \"grpo/loss\": loss.item(),\n            \"grpo/advantage_chosen\": advantages[:, 0].mean().item(),\n            \"grpo/num_rejected\": rejected_logps.size(1)\n        })\n\npolicy.save_pretrained(\"models/grpo_lora\")\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T11:15:38.393768Z","iopub.execute_input":"2026-01-01T11:15:38.394700Z","iopub.status.idle":"2026-01-01T11:15:38.400822Z","shell.execute_reply.started":"2026-01-01T11:15:38.394666Z","shell.execute_reply":"2026-01-01T11:15:38.400210Z"}},"outputs":[{"name":"stdout","text":"Writing rlhf/grpo_train.py\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"!python -m rlhf.grpo_train.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T11:15:50.967200Z","iopub.execute_input":"2026-01-01T11:15:50.967504Z","iopub.status.idle":"2026-01-01T11:16:13.452982Z","shell.execute_reply.started":"2026-01-01T11:15:50.967477Z","shell.execute_reply":"2026-01-01T11:16:13.452337Z"}},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n2026-01-01 11:15:57.474707: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767266157.495787    1273 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767266157.502296    1273 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767266157.519005    1273 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767266157.519037    1273 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767266157.519042    1273 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767266157.519049    1273 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrahulkrish28\u001b[0m (\u001b[33mrahulkrish28-california-state-university-fullerton\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20260101_111601-2vgl634f\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mGRPO-LoRA-Multi\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/rahulkrish28-california-state-university-fullerton/llm-from-scratch\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/rahulkrish28-california-state-university-fullerton/llm-from-scratch/runs/2vgl634f\u001b[0m\n/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n  warnings.warn(\nGenerating train split: 5 examples [00:00, 594.48 examples/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.14it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.32it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.32it/s]\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m updating run metadata (0.0s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m updating run metadata (0.0s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m updating run metadata (0.0s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m updating run metadata (0.0s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m updating run metadata (0.0s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m: grpo/advantage_chosen ‚ñÇ‚ñÖ‚ñá‚ñÉ‚ñÖ‚ñá‚ñÖ‚ñÅ‚ñà\n\u001b[34m\u001b[1mwandb\u001b[0m:             grpo/loss ‚ñá‚ñÑ‚ñÇ‚ñÜ‚ñÑ‚ñÇ‚ñÑ‚ñà‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:     grpo/num_rejected ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m: grpo/advantage_chosen -2.29123\n\u001b[34m\u001b[1mwandb\u001b[0m:             grpo/loss 2.29123\n\u001b[34m\u001b[1mwandb\u001b[0m:     grpo/num_rejected 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mGRPO-LoRA-Multi\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rahulkrish28-california-state-university-fullerton/llm-from-scratch/runs/2vgl634f\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/rahulkrish28-california-state-university-fullerton/llm-from-scratch\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20260101_111601-2vgl634f/logs\u001b[0m\n/usr/bin/python3: Error while finding module specification for 'rlhf.grpo_train.py' (ModuleNotFoundError: __path__ attribute not found on 'rlhf.grpo_train' while trying to find 'rlhf.grpo_train.py'). Try using 'rlhf.grpo_train' instead of 'rlhf.grpo_train.py' as the module name.\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"%%writefile inference/grpo_inference.py\n\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\nfrom rlhf.utils import sequence_logprob\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# -------------------------\n# Load tokenizer\n# -------------------------\ntokenizer = AutoTokenizer.from_pretrained(\"outputs/train_lm\")\ntokenizer.pad_token = tokenizer.eos_token\n\n# -------------------------\n# Load base (SFT) model\n# -------------------------\nbase_model = AutoModelForCausalLM.from_pretrained(\"outputs/train_lm\").to(device)\nbase_model.eval()\n\n# -------------------------\n# Load GRPO policy (base + LoRA)\n# -------------------------\npolicy_model = AutoModelForCausalLM.from_pretrained(\"outputs/train_lm\")\npolicy_model = PeftModel.from_pretrained(policy_model, \"models/grpo_lora\")\npolicy_model = policy_model.to(device)\npolicy_model.eval()\n\n# -------------------------\n# Helper: generate\n# -------------------------\ndef generate(model, prompt, max_new_tokens=100):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        do_sample=True,\n        temperature=0.7,\n        top_p=0.9\n    )\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# -------------------------\n# GRPO score (group advantage)\n# -------------------------\ndef grpo_advantage(model, chosen_text, rejected_texts):\n    # chosen\n    chosen = tokenizer(chosen_text, return_tensors=\"pt\", truncation=True).to(device)\n    lp_chosen = sequence_logprob(\n        model,\n        chosen[\"input_ids\"],\n        chosen[\"attention_mask\"]\n    )\n\n    # rejected group\n    rejected_logps = []\n    for txt in rejected_texts:\n        r = tokenizer(txt, return_tensors=\"pt\", truncation=True).to(device)\n        lp_r = sequence_logprob(\n            model,\n            r[\"input_ids\"],\n            r[\"attention_mask\"]\n        )\n        rejected_logps.append(lp_r)\n\n    rejected_logps = torch.stack(rejected_logps, dim=1)  # [B=1, K]\n\n    all_scores = torch.cat(\n        [lp_chosen.unsqueeze(1), rejected_logps],\n        dim=1\n    )\n\n    advantages = all_scores - all_scores.mean(dim=1, keepdim=True)\n    return advantages[:, 0].item()  # chosen advantage\n\n# -------------------------\n# Example inference\n# -------------------------\nprompt = \"Explain why reinforcement learning is useful for aligning large language models.\"\n\n# Generate responses\nbase_text = generate(base_model, prompt)\npolicy_text = generate(policy_model, prompt)\n\nprint(\"\\n=== PROMPT ===\")\nprint(prompt)\n\nprint(\"\\n=== BASE MODEL ===\")\nprint(base_text)\n\nprint(\"\\n=== GRPO MODEL ===\")\nprint(policy_text)\n\n# -------------------------\n# Group comparison\n# -------------------------\nrejected_examples = [\n    base_text,\n    \"I don't know.\",\n    \"Reinforcement learning is not important.\"\n]\n\nadv = grpo_advantage(\n    policy_model,\n    chosen_text=policy_text,\n    rejected_texts=rejected_examples\n)\n\nprint(\"\\n=== GRPO ADVANTAGE ===\")\nprint(f\"Chosen advantage over group: {adv:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T11:18:00.561320Z","iopub.execute_input":"2026-01-01T11:18:00.561723Z","iopub.status.idle":"2026-01-01T11:18:00.568998Z","shell.execute_reply.started":"2026-01-01T11:18:00.561688Z","shell.execute_reply":"2026-01-01T11:18:00.568344Z"}},"outputs":[{"name":"stdout","text":"Writing inference/grpo_inference.py\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"!python -m inference.grpo_inference.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T11:18:14.793311Z","iopub.execute_input":"2026-01-01T11:18:14.793870Z","iopub.status.idle":"2026-01-01T11:18:29.891304Z","shell.execute_reply.started":"2026-01-01T11:18:14.793843Z","shell.execute_reply":"2026-01-01T11:18:29.890413Z"}},"outputs":[{"name":"stdout","text":"2026-01-01 11:18:20.454421: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767266300.475946    1328 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767266300.482599    1328 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767266300.499325    1328 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767266300.499353    1328 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767266300.499356    1328 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767266300.499359    1328 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n=== PROMPT ===\nExplain why reinforcement learning is useful for aligning large language models.\n\n=== BASE MODEL ===\nExplain why reinforcement learning is useful for aligning large language models. \n\nA model of reinforcement learning , which requires no explicit information about the input or output of a task , is called a model of reinforcement learning . A model of reinforcement learning is a supervised learning process in which a model is trained on a set of input and output conditions . \n\nA model of reinforcement learning is a supervised learning process in which a model is trained on a set of input and output conditions . \n\nA model of reinforcement learning is a supervised learning process in which a\n\n=== GRPO MODEL ===\nExplain why reinforcement learning is useful for aligning large language models. \n\n \n\nA recent paper by Daniel Kahneman , Richard Heylin , and others , which was published in 2007 , was based on a study by Daniel Kahneman , who had used reinforcement learning to train human subjects in a series of tests on the development of the theory of reinforcement learning . \n = \n\nMathematicians have shown that reinforcement learning is a critical component of AI , in particular , the reinforcement learning process . \n = \n\nMathematicians\n\n=== GRPO ADVANTAGE ===\nChosen advantage over group: -142.8555\n/usr/bin/python3: Error while finding module specification for 'inference.grpo_inference.py' (ModuleNotFoundError: __path__ attribute not found on 'inference.grpo_inference' while trying to find 'inference.grpo_inference.py'). Try using 'inference.grpo_inference' instead of 'inference.grpo_inference.py' as the module name.\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}