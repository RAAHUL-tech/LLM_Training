model_name: gpt2
dataset_name: wikitext
dataset_config: wikitext-2-raw-v1

max_length: 512
batch_size: 2
num_epochs: 1

learning_rate: 5e-5
weight_decay: 0.01
warmup_ratio: 0.1
max_grad_norm: 1.0

output_dir: outputs/train_lm
seed: 42