# LLM Training From Scratch â€” Systems + RLHF

End-to-end implementation of **LLM pretraining, fine-tuning, and RLHF** with a strong focus on **systems correctness**, **memory efficiency**, and **modern preference optimization methods (DPO / GRPO / PPO)**.

This repository is designed to be:
- âœ… **Single-GPU friendly** (Kaggle / Colab)
- âœ… **Distributed-correct** (CPU DDP simulation)
- âœ… **Industry-aligned** (LoRA, AMP, GRPO, logging, inference optimization)

---

## ğŸ“Œ What This Project Covers

### 1ï¸âƒ£ Language Model Training
- Causal LM training from HuggingFace models
- AMP (mixed precision)
- Gradient checkpointing
- Distributed Data Parallel (DDP)
- LoRA fine-tuning

### 2ï¸âƒ£ RLHF & Preference Optimization
- Preference dataset generation
- Reward modeling
- PPO (policy gradient RLHF)
- DPO (Direct Preference Optimization)
- GRPO (Group-based preference optimization, multi-rejection)

### 3ï¸âƒ£ Inference & Deployment
- Batched inference
- LoRA loading
- DPO / PPO / GRPO inference scripts
- CPU & GPU compatible

### 4ï¸âƒ£ Experiment Tracking
- Weights & Biases logging
- Loss, reward, KL, advantage tracking

---

## ğŸ“‚ Project Structure

```

llm-from-scratch/
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ train_base.yaml          # Training hyperparameters
â”‚   â””â”€â”€ inference.yaml           # Inference settings
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ prompts.json             # Base prompts
â”‚   â”œâ”€â”€ generate_preferences.py  # Pairwise preference generation
â”‚   â””â”€â”€ generate_preferences_multi.py  # Multi-rejection (GRPO)
â”‚
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ train_lm.py               # Baseline LM training
â”‚   â”œâ”€â”€ train_lm_ddp.py           # DDP training (CPU/GPU)
â”‚   â”œâ”€â”€ train_lm_amp_ckpt.py      # AMP + checkpointing
â”‚   â”œâ”€â”€ train_lm_lora.py          # LoRA fine-tuning
â”‚   â””â”€â”€ train_lm_wandb.py         # Training with W&B logging
â”‚
â”œâ”€â”€ rlhf/
â”‚   â”œâ”€â”€ dataset.py                # Preference datasets
â”‚   â”œâ”€â”€ reward_model.py           # Reward model definition
â”‚   â”œâ”€â”€ train_reward_model.py     # Reward model training
â”‚   â”œâ”€â”€ dpo_train.py              # DPO training
â”‚   â”œâ”€â”€ ppo_train.py              # PPO training
â”‚   â”œâ”€â”€ grpo_train.py             # GRPO training (multi-rejection)
â”‚   â””â”€â”€ utils.py                  # Logprob utilities
â”‚
â”œâ”€â”€ inference/
â”‚   â”œâ”€â”€ inference.py              # Base inference
â”‚   â”œâ”€â”€ dpo_inference.py          # DPO inference
â”‚   â”œâ”€â”€ ppo_inference.py          # PPO inference
â”‚   â””â”€â”€ grpo_inference.py         # GRPO inference
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ llm-training.ipynb        # Interactive experimentation
â”‚
â”œâ”€â”€ models/                       # Saved checkpoints
â”œâ”€â”€ report/                       # Final report & results
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â””â”€â”€ .gitignore

````

---

## Getting Started

### 1ï¸âƒ£ Environment Setup

```bash
pip install -r requirements.txt
````

Minimum dependencies:

* `torch`
* `transformers`
* `datasets`
* `peft`
* `wandb`
* `accelerate`

---

## âš™ï¸ Configuration

### `configs/train_base.yaml`

Controls:

* model name
* batch size
* max sequence length
* learning rate
* epochs

Example:

```yaml
model_name: gpt2
batch_size: 4
max_length: 512
lr: 1e-5
epochs: 3
```

---

## Training

### ğŸ”¹ Baseline LM Training

```bash
python train/train_lm.py
```

### ğŸ”¹ DDP (CPU/GPU Safe)

```bash
torchrun --nproc_per_node=2 train/train_lm_ddp.py
```

### ğŸ”¹ AMP + Checkpointing

```bash
python train/train_lm_amp_ckpt.py
```

### ğŸ”¹ LoRA Fine-Tuning

```bash
python train/train_lm_lora.py
```

---

## RLHF Pipeline

### 1ï¸âƒ£ Generate Preference Data

#### Pairwise (DPO / PPO)

```bash
python data/generate_preferences.py
```

#### Multi-Rejection (GRPO)

```bash
python data/generate_preferences_multi.py
```

---

### 2ï¸âƒ£ Train Reward Model

```bash
python rlhf/train_reward_model.py
```

---

### 3ï¸âƒ£ Preference Optimization

#### ğŸ”¹ DPO

```bash
python rlhf/dpo_train.py
```

#### ğŸ”¹ PPO

```bash
python rlhf/ppo_train.py
```

#### ğŸ”¹ GRPO (Multi-Rejection)

```bash
python rlhf/grpo_train.py
```

âœ” No reward model required
âœ” Group-normalized advantages
âœ” Lower variance than PPO

---

## ğŸ§ª Inference

### Base

```bash
python inference/inference.py
```

### DPO / PPO / GRPO

```bash
python inference/dpo_inference.py
python inference/ppo_inference.py
python inference/grpo_inference.py
```

Supports:

* LoRA adapters
* Batched decoding
* CPU & GPU

---

## ğŸ“Š Experiment Tracking (W&B)

All training scripts support:

* Loss curves
* Reward / advantage
* KL divergence (PPO)
* Throughput

```bash
wandb login
```

Runs are logged under:

```
project = "llm-from-scratch"
```

---

## ğŸ“Œ Future Extensions

* FlashAttention
* Quantized inference (4-bit)
* Tensor parallelism (Megatron-style)
* Safety & bias evaluation
* Model evaluation (MAUVE / BERTScore)

---

## ğŸ“œ License

MIT License â€” free to use, modify, and learn from.

---
```
```
